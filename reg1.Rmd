---
#title: "Supervised Machine Learning: Linear Models (Regression)"
#author: "Dr. Stephan Dietrich & Michelle González Amador"
#date: '2022-09-21'
output: html_document
---
<style>
    body {
    text-align: justify}
</style>

## **Supervised Machine Learning: Linear Models (Regression)** {.tabset .tabset-fade .tabset-pills}

**Introducing the Prediction Policy Framework**

In the video-lecture below you'll be given a brief introduction to the prediction policy framework, and a primer on machine learning. Please take a moment to watch the 10 minute video.

<center>
```{r, echo=FALSE}
library("vembedr")
embed_url("https://www.youtube.com/watch?v=aA7tLQxf1xE")
```
</center>

After watching the video, we have a practical excercise in R:

**Predicting social assistance beneficiaries** 

Social policies usually work with tight budgets and limited fiscal space. To allocate resources efficiently, benefits need to be targeted to those who need them the most. Yet, identifying needs isn’t easy and misclassifications can have severe and irreversible impacts on people in need. Different targeting mechanisms are being used, including expert judgment to define eligibility criteria or selection by community committees. The most common approach for anti-poverty programs is based on data: Proxy Means Testing (PMT). A recent review and discussion of social assistance targeting by the World Bank can be found [here](https://www.worldbank.org/en/topic/socialprotection/publication/a-new-look-at-old-dilemmas-revisiting-targeting-in-social-assistance).

<center>
```{r, echo=FALSE}
library("vembedr")
embed_url("https://www.youtube.com/watch?v=Tzm9r91DBnc")
```
</center>

The underlying idea behind PMT is to use data on easily observable household characteristics - which are hard to manipulate - to predict poverty. Traditionally, regressions of observable characteristics on reported consumption are performed on existing large-scale survey data sets. The regression coefficients of the best-performing model are then used as a weight. To define program eligibility, a second survey is conducted with potential beneficiaries. To screen households, information on the selected PMT variables is collected (e.g. number of rooms, type of house, household size) and the values of the chosen observables are multiplied by the observables' respective weight (derived from the previous regression). Households are then ordered according to their predicted PMT score and benefits are allocated to households below a certain threshold. 
In block 1 of this course, we use data to build a PMT algorithm. We rely on World Bank’s LSMS household survey data from Malawi to build a model to rank and identify households in need. Before jumping to the data, we need to ensure that we have a proper understanding of the problem we want to solve.

**Discussion Points**

<ul>
<li> Why is this a prediction policy problem? What would be a causal inference problem in this setting? Is it a regression or a classification problem? </li>
<li> Which variables and characteristics we include in the PMT can make a big difference. 
    <ul>
    <li>Programmatically and conceptually, which type of characteristics do we want to consider for the PMT?</li>
    <li>Technically, how do we select which variables to include in a prediction model? How is this difference from a causal inference problem?</li>
    </ul>
</li>    
<li> What are the practical implications of the bias-variance tradeoff in this application? </li>
<li> What are potential risks of such a data driven targeting approach? </li>
</ul>

<br>


### **Merging datasets with R: LSMS data example**

Many organisations, institutions, and firms around the world collect and, if we're lucky, share data. One such example is The World Bank Group who, in cooperation with local statistical offices, collects cross-sectional and panel data on the socioeconomic lives of country nationals. The Living Standards Measurement Survey (LSMS, for short) has been around since the 1980's, and it is publicly shared in various formats and distinct files. We've chosen to download the 2019-2020 LSMS data for Malawi, a cross-sectional database which contains several modules on Health, Education, Food Security, etc. When you download the data, you receive a folder with several .csv files that must be put together. The code below does just that.

**1. Preliminaries: setting working directory and opening libraries**

```{r, message=FALSE}
# 1.1 Cleaning the working environment and setting up the working directory path
rm(list = ls())
#setwd("~/Desktop/Malawi2019")
# 1.2 Opening libraries

#install.packages("tidyverse","data.table")
library(tidyverse)
library(data.table)

```

**2. Uploading the data**

We will be working with the Malawi dataset from the LSMS surveys from the World Bank Group. To download and merge the data, you can refer to the documentation: [World Bank Microdata: LSMS Malawi](https://microdata.worldbank.org/index.php/catalog/3818/related-materials).

```{r}
# The following like will look inside the folder defined by file.path() command and collect the names of all the files that contain the word patterns "hh_mod". 
filedir <- file.path("~/Desktop/Malawi2019")
filenames <- list.files(filedir,pattern="hh_mod", full.names=TRUE, recursive=FALSE, ignore.case=TRUE)

# data object now contains all files from the 2019 round from the Household Module
print(filenames)

# We will now create function that will read all the files referenced in the filenames object and assign an an object to them (each file will become an element within a list).

datList <- lapply(filenames, function(i) {
    df <- read.csv(i, header=TRUE, strip.white=TRUE)
    return(df)
}) # this function should read all the *listed* files from above, convert them into a data frame and store them in a list

head(datList[[1]]) # returns the first elements in the first dataframe from the list datList

# A smart thing to do is to clean your Global Environment every so often when you are done with an object, to free up some memory
rm(filenames, filedir)

```


Note that the order of the datasets within datList follows the order of the names in filenames, such that dataset hh_mod_a_filt.csv would be 1,  HH_MOD_B.csv would be 2, and so forth.

**3. Merging data **

Now that we have succesfully uploaded all the files from our folder, we want to merge the datasets stored in the list (datList):

- We want to merge the first dataset [module A], containing household level information, with module B, a Household Roster with Individual level information. The variable *case_id* is the unique identifier at the household level, and it is present in both datasets.

```{r}

datAB <- merge(datList[[1]], datList[[2]], by="case_id", all = TRUE)

```

Let's do some sanity checks to see whether our merging worked as intended:

```{r}

# number of rows and columns of module A
dim(datList[[1]]) # 11434 rows, 21 columns
# number of rows and columns of module B
dim(datList[[2]]) # 50476 rows, 53 columns
# number of rows and columns of our merged data. Rows MUST be the same as module B
dim(datAB) # 50476 rows # 73 columns
# notice that the number of columns = Cols A + Cols B -1[variable we used to merge]

#Brief overview of our new dataset/dataframe!
str(datAB)
```

Something that you might find interesting is that the numeric variable *case_id* (which we used to merge) looks odd. This is due to the fact that R is reading this number in scientific form, if you'd like to see the standard number:

```{r}
options(scipen = 100) # run this line to turn off scientific form, to set it back use options(scipen = 0)

```

Now you can run the *str(datAB)* line again and see the *case_id* variable in standard form.
Let's further merge dataframe datAB with module C, which contains individual level data on Education. The individual unique identifier, which exists in Module B, and C (and now also in datAB after the merge) is *PID*:

```{r}
datABC <- merge(datAB, datList[[3]], by=c("case_id","PID"), all = TRUE)

```

We use the household unique identifier, *case_id*, to match households and the individual unique identifier, *PID*, to match individuals within each household. The dataframe should increase in column number, but not in number of observations (feel free to check using dim() or in the global environment).# We should select other relevant modules from the IHS5 Household Database: D[Health; case_id PID], E[Time Use & Labour; case_id PID], F[Housing, case_id], H[Food Security, case_id], T[Subjective Assessment of Well-Being, case_id]:

```{r}

datABCD <- merge(datABC, datList[[4]], by=c("case_id","PID"), all = TRUE)
datABCDE <- merge(datABCD, datList[[5]], by=c("case_id","PID"), all = TRUE)

```

There's a warning! It isn't a big deal (the code still did it's job), but let's try to sort it out anyway. Dataframe datABCD contains already variables HHID.x and HHID.y. Dataframe datABCDE now contains HHID, HHID.x and HHID.y, so perhaps it would be prudent to delete two of the three duplicates:

```{r}

# let's find out the position of these two columns in the dataframe
which( colnames(datABCDE)=="HHID.x") # 3 and 74
which( colnames(datABCDE)=="HHID.y") # 23 and 129
which( colnames(datABCDE)=="HHID") # 187 (let's keep this guy)
datABCDE <- datABCDE[,-c(3,23,74,129)] # this line removes the four selected duplicated columns.

```
Note that as we continue to merge multiple modules these duplicated columns will reappear. This happens when two datasets containing the same vector name <besides the unique identifier> are merged.

```{r}
#Merging Module F (Housing)
datABCDEF <- merge(datABCDE, datList[[6]], by="case_id", all = TRUE)
```

Up until this point, the order of datList dataframes and Modules was the same (1-A,2-B,3-C, etc.). Now, we need to take into account that there exist modules F1, G1, G2 etc.You can refer to Table 10: Structure of the IHS5 Household Database of the [Basic Information Document](https://microdata.worldbank.org/index.php/catalog/2939/related-materials) to confirm the correct order.


```{r}
datABCDEFH <- merge(datABCDEF, datList[[11]], by="case_id", all = TRUE)
datABCDEFHT <- merge(datABCDEFH, datList[[28]], by="case_id", all = TRUE)
which( colnames(datABCDEFHT)=="HHID.x") # 183 454
which( colnames(datABCDEFHT)=="HHID.y") # 340 494
datABCDEFHT <- datABCDEFHT[,-c(454,340,494)]
names(datABCDEFHT)[names(datABCDEFHT) == "HHID.x"] <- "HHID" # change the name of the non-eliminated HHID.x column to HHID

```

We should also include some information on consumption:

```{r}
# G1[Food Consumption Over Past One Week; case_id hh_g02] 8
# This module collects information on all food consumed by the household in the past 7 days
datG1 = datList[[8]]

datG1 <- datG1[,c(1,7,8)] # I only want to keep the unique household identifier, item code,  and the total amount consumed in the past week.
# We want to create a vector (variable) that contains one single number: the total amount spent on food in the past 7 days.

# Initialise empty vectors for 'for' loop
list <- unique(datG1$case_id) # list is an object containing only
idx  <- NA
hold <- NA
sumConsumption <- NA
temp <- NA
data <- NA

for (i in 1:length(list)) { # iterate over the number of elements in object 'list' 
        
        idx    <- which(datG1$case_id == list[i]) #index containing the unique id that matches the iteration number
        hold   <- datG1[idx,] # vector containing all the elements that match the previous indicator
        sumConsumption  <- sum(hold$hh_g03a, na.rm = TRUE) # sum all the values contained in food item consumption (hh_g03a) vector (in $)
        
        temp <- cbind(hold$case_id,sumConsumption) # create a temporary object with the two concatenated vectors: the unique identifier and the final consumption variable
        data  <- rbind(data,temp) # append the temporary object to dataframe data
        
}

# Make sure that the object data is stored correctly: dataframe type
datG <- as.data.frame(data)

# let's keep only one observation per household
datG <- unique(datG)
# Now let's rename the variable and merge our consumption data!
names(datG)[names(datG) == "V1"] <- "case_id"
# The first row is empty, let's get rid of it
datG <- datG[-1,]

datABCDEFHTG1 <- merge(datABCDEFHT, datG, by="case_id", all = TRUE)

# save/export the final, merged dataset
write_rds(datABCDEFHTG1, "Malawi_2019(2).rds","xz", compression = 9L)

```

You can download the final data set by clicking on the button below.
```{r, echo=FALSE}
library(downloadthis)
download_link(
  link = "https://github.com/michelleg06/Machine-Learning-for-Public-Policy/blob/main/Malawi_2019.rds",
  button_label = "Download merged file (.rds)",
  button_type = "info",
  has_icon = TRUE,
  icon = "fa fa-save",
  self_contained = FALSE
)

```

### **Machine Learning with linear regression**

This code is a step by step on how to go about writing a predictive model using a *linear regression*. Despite its simplicity and transparency, i.e. the ease with which we can interpret its results, a linear model is not without challenges in machine learning. Using the Malawi 2019-2020 LSMS data, we will predict a known outcome, *Food Consumption*, by training our model using correlates (i.e. features) that would explain the phenomenon; i.e. we will build a PMT algorithm.

**1. Preliminaries: working directory, libraries, data upload**

```{r, message=FALSE}
rm(list = ls())
#setwd("~/Desktop/MachineLearning4PP/Machine-Learning-for-Public-Policy")

# Libraries

# install.packages("caret", "corrplot", "Hmisc", "plyr")
library(plyr)
library(tidyverse)
library(data.table)
library(caret)
library(corrplot)
library(Hmisc)
library(stargazer)

malawi <- read_rds("/Users/michellegonzalez/Documents/GitHub/Machine-Learning-for-Public-Policy/Malawi_2019.rds")
```

**2. Data Pre-Processing: data visualisation and data wrangling**

The Malawi dataset contains 50,476 observations and 514 features (variables). It is a very large dataframe, and we want to work with only a subset of it using all the observations but only some features; i.e. we're going to create a subset of the dataframe that contains only the relevant features for our PMT.You can find the data label descriptions [here](https://microdata.worldbank.org/index.php/catalog/3818/data-dictionary).


```{r}
dim(malawi)
```

We should begin the *critical* process of feature selection (or variable selection). Some notes on this: from a practical point of view, a model with less predictors may be easier to interpret. Also, some models may be negatively affected by non-informative predictors. This process is similar to traditional econometric modeling, but we should not conflate predictive and explanatory modeling. Importantly, please note that we are not interested in knowing why something happens, but rather in what is likely to happen given some known data. Hence: 

*Target variable* (a.k.a. Dependent Variable): Total Food Consumption \$ in the Past 7 Days

Which variables would help us predict Malawian households' weekly food-spending?

```{r, results=FALSE}
# By looking at the data dictionary, I've selected the following variables:

#1 $ food consumption in the past week: sumConsumption
which( colnames(malawi)=="sumConsumption") # 514
#2 How old is NAME (year): hh_b05a
which( colnames(malawi)=="hh_b05a") # 27
#3 reside: reside
which( colnames(malawi)=="reside") # 6
#4 household size: hhsize
which( colnames(malawi)=="hhsize") #21 
#5 did you or anyone else in this HH borrow on credit?: hh_s01
which( colnames(malawi)=="hh_s01") # 19
#6 gender [1 MALE, 2 FEMALE]: hh_b03 
which( colnames(malawi)=="hh_b03") # 22
#7 What is the highest educational qualification [NAME] has acquired: hh_c09
which( colnames(malawi)=="hh_c09") # 88
#8 $ Tuition, including any extra tuition fees (hh_c22a)
which( colnames(malawi)=="hh_c22a") # 105
#9 $ Expenditures on after school programs & tutoring (hh_c22b)
which( colnames(malawi)=="hh_c22b") # 106
#10 $ School books and other materials (hh_c22c)
which( colnames(malawi)=="hh_c22c") # 107
#11 $ School uniform clothing (hh_c22d)
which( colnames(malawi)=="hh_c22d") #108
#12 $ TOTAL (hh_c22j)
which( colnames(malawi)=="hh_c22j") #114
#13 How much was spent in TOTAL in the last 12 months? (hh_c22l)
which( colnames(malawi)=="hh_c22l") #116
#14 Have you used a computer? (hh_c24)
which( colnames(malawi)=="hh_c24") # 124
#15 Amt [NAME] spent in the past 4 weeks for all illnesses and injuries? (hh_d10)
which( colnames(malawi)=="hh_d10") # 142
#16 Amt in total did [NAME]spent...for medical care not related to an illness? (hh_d11)
which( colnames(malawi)=="hh_d11") #143 
#17 How much in total did [NAME] spend..for medical insurance? (hh_d12_1)
which( colnames(malawi)=="hh_d12_1") #145
#18 Does [NAME] want to change his/her current employment situation? (hh_e70)
which( colnames(malawi)=="hh_e70") #337
#19 What was the total amout paid in the form of land rent during the p... (hh_f04_4)
which( colnames(malawi)=="hh_f04_4") #355
#20 hh_f04a How much do you pay to rent this property?
which( colnames(malawi)=="hh_f04a") #358
#21 Do you have electricity working in your dwelling? (hh_f19)
which( colnames(malawi)=="hh_f19") #381
#22 How many working cell phones in total does your household own? (hh_f34)
which( colnames(malawi)=="hh_f34") #404
#23 Which of the following is true? Your current income . . . (hh_t08)
which( colnames(malawi)=="hh_t08") #499
#24 Concerning your HH's food consumption, over the past months which is true (hh_t01)
which( colnames(malawi)=="hh_t01") #492
#25 HH were unable to eat healthy & nutritious food b'se of a lack of money/other (hh_t14)
which( colnames(malawi)=="hh_t14") #507
#26 ... What other features do you consider important?

# subsetting our dataframe
cols   <- c(6,21,27,514,19,22,88,105,106,107,108,114,116,124,142,143,145,337,355,358,381,404,499,492,507)
malawi <- malawi[,cols] 
# The malawi dataframe should only have 25 (selected) features now
# Also know that you can subset with the column name instead of the number of the column. In this exercise we use the number as a way to introduce the which() function, which may come in handy in the future. 
```

*Missing values*

In traditional econometric models, we often discuss what we can do in the presence of missing data. First and foremost, we should assess whether there is a pattern to missingness and if so, what that means to what we can learn from our (sub)population. This remains true for machine learning. A practical difference is that while one can implement a causal econometric model with missing values, many machine learning models fail if there are any in the training or control data. 

```{r}
# Missing values (NAs) by feature
colSums(is.na(malawi))
```

Interestingly, a lot of the features from module C have more than half the values missing (>30,000 hhs). All of the variables refer to \$ spent on education. For the sake of simplicity, I will simply remove those features. But this action may later affect our predictive model, if such features were to increase its predictive performance. Another alternative would be to only work with the remaining households for which we have full data. However, that would reduce our sample size significantly and I would discourage it. Finally, multiple imputation is not feasible due to the [large share of missingness in the data](https://gist.github.com/farrajota/a733524a814596b0124d068a55221c29). 

```{r}
#hh_c22a-l
which( colnames(malawi)=="hh_c22a") # 8
which( colnames(malawi)=="hh_c22b") # 9
which( colnames(malawi)=="hh_c22c") # 10
which( colnames(malawi)=="hh_c22d") #11
which( colnames(malawi)=="hh_c22j") #12
which( colnames(malawi)=="hh_c22l") #13

cols_cModule <- c(8,9,10,11,12,13)
malawi <- malawi[,-cols_cModule]
# we have 19 features left in the malawi dataframe

# Are there any remaining missing values? double-checking is always a good idea!
colSums(is.na(malawi))
# There seem to be quite a lot of missing values in a couple of vectors from the F module
which( colnames(malawi)=="hh_f04_4") # 13
which( colnames(malawi)=="hh_f04a") # 14

malawi <- malawi[,-c(13,14)]

# The target feature, Food Consumption in the past 7 days [sumConsumption], has 9046 missing values. Let's get rid of them
rows_consumption <- which(is.na(malawi$sumConsumption))
head(rows_consumption)
tail(rows_consumption)
length(rows_consumption) # we've created an indicator with all the missing rows

malawi <- malawi[-rows_consumption,]
colSums(is.na(malawi)) # all clear!
```

*Visualising our data*

A quick and effective way to take a first glance at our data is to plot **histograms** of relevant (continuously distributed) features.

```{r}
malawi_continuous <- malawi %>% select_if(~is.integer(.) | is.numeric(.)) # this line selects all variables in the dataframe which are integer OR numeric, and can therefore be plotted as a histogram.
hist.data.frame(malawi_continuous) # from the Hmisc package, quick and painless.
```

Some important information we can gather from these plots: our food consumption measure is skewed (perhaps it should be logged?), there are a number of variables for which there seems to be bunching at the zero value. The only (close to) normal distribution we observe is in terms of household size.

We should plot the remaining factor (binary, categorical) features.

```{r}
malawi_factor <- malawi %>% select_if(~is.factor(.)) # subset of the dataframe containing only factor variables
llply(.data=malawi_factor, .fun=table) # create tables of all the variables in dataframe using the plyr package
```

Some of what we can gather from looking at the tables is that there are features for which the levels (also known as categories) are not labeled in a way that we can understand. For example, feature hh_c09 (What is the highest educational qualification [NAME] has acquired) has about 10,012 values under an unnamed educational qualification. Two things may be going on here: 1) the enumerators coded missing values differently from NA (which R automatically reads as missing), or 2) there is a value which was unlabeled and we may need to relabel. 

```{r}
levels(malawi$hh_c09)
# the line above tells us that there are empty cells, recognised by the level "".
head(malawi$hh_c09) # we can confirm that it is indeed an empty cell by looking at the first six values of the dataframe (using the head command, which returns the first six elements). One of the six elements is nothing. 
```

The main take away from creating tables of all the factor variables in the dataframe is that there are still missing values that were not taken care of during the data cleaning step. This is due to the fact that, as R read the factors, it mistakenly read an empty cell as a category, and not a missing value. 

*Exploring relationships between features*

Let's explore the relationships that may or may not exist between our selected features using two distinct correlation matrices.

```{r, figures-side, fig.show="hold", out.width="50%"}
M <- cor(malawi_continuous) # create a correlation matrix of the continuous dataset, cor() uses Pearson's correlation coefficient as default. This means we can only take the correlation between continuous variables
corrplot(M, method="circle", addCoef.col ="black", number.cex = 0.8) # visualise it in a nice way

# Let's compute the Spearman correlation coefficient between categorical variables
malawi_factorC <- as.data.frame(lapply(malawi_factor,as.numeric)) # coerce dataframe to numeric, as the cor() command only takes in numeric types

# add the target variable to the factor correlation matrix
malawi_factorC <- cbind.data.frame(malawi_factorC,malawi_continuous$sumConsumption)
names(malawi_factorC)[names(malawi_factorC) == "malawi_continuous$sumConsumption"] <- "sumConsumption"

M2 <- cor(malawi_factorC, method = "spearman")
corrplot(M2, method="circle", addCoef.col ="black", number.cex = 0.7) # visualise it in a nice way

```

Notice how, in both matrices, but particularly that which contains our target variable, we observe several instances were the correlation coefficient is zero. We call this, a zero-variance predictor. For many machine learning models (excluding tree-based models), this may cause the model to crash or the fit to be unstable.

**3. Partition data into training and test datasets**

When we are working with machine learning learning models, it is important to have two data sets. A training data set from which our model will learn, and a test data set containing the same features as our training data set. To split our main data set into two, we will work with an 80/20 split. 

The 80/20 split has its origins in the Pareto Principle, which states that "in most cases, 80% of effects from from 20% of causes". Without other relevant knowledge of the source or shape of the data, this partitioning method is a good place to start and indeed standard in the machine learning field.


```{r}
# Always set a seet for reproducibility purposes!
set.seed(1234777) # use any number you want

# We could split the data manually, but the caret package includes an useful function

train_idx <- createDataPartition(malawi$sumConsumption, p = .8, list = FALSE, times = 1)
head(train_idx)

Train_df <- malawi[ train_idx,]
Test_df  <- malawi[-train_idx,]
```

**4. Creating our first predictive model and evaluating its performance: RMSE and R^2**

We will start by fitting a predictive model using the training dataset; that is, our target variable *Total household food consumption in the past 7 days* or *sumConsumption* will be a $Y$ dependent variable in a linear model $Y_i = \alpha + x'\beta_i + \epsilon_i$, and the remaining features in the data frame correspond to the row vectors $x'\beta$. 

```{r, echo = TRUE, warning = FALSE}
model1 <- lm(sumConsumption ~ .,Train_df) # the dot after the squiggle ~ asks the lm() function tu use all other variables in the dataframe as predictors to the dependent variable sumConsumption
stargazer(model1, type = "text")
```

The output of our model, obtained with the summary() command, or stargazer(), has three important indicators to assess the performance of our model:

- Model *residuals*: recall residuals are the observed value minus the predicted value.

```{r, echo=FALSE}
print(summary(model1$residuals))
```

The max(imum) error of $3923.07$ suggests that the model under-predicted expenses by circa \$4,000 for at least one observation. Fifty percent of the predictions (between the first and third quartiles) over-predict the true consumption value by \$76.74 and \$12.11. From these data, we obtain the popular measure of performance evaluation known as the Root Mean Squared Error (RMSE, for short).

```{r, warning=FALSE}
# Calculate the RMSE for the training dataset, or the in-sample RMSE.

# 1. Predict values on the training dataset
p0 <- predict(model1, Train_df)

# 2. Obtain the errors (predicted values minus observed values of target variable)
error0 <- p0 - Train_df[["sumConsumption"]]

# 3. In-sample RMSE
RMSE_insample <- sqrt(mean(error0 ^ 2))
print(RMSE_insample)
```

The RMSE ($228.4205$) gives us an absolute number that indicates how much our predicted values deviate from the true (observed) number. Think of the question, *how far, on average, are the residuals away from zero?* Generally speaking, the lower the value, the better the model fit. Besides being a good measure of goodness of fit, the RMSE is also useful for comparing the ability of our model to make predictions on different (e.g. test) data sets. The in-sample RMSE should be close or equal to the out-of-sample RMSE.

- The *p-values*: represented by stars *** indicate the predictive power of each feature in the model. In the same line, the magnitude of the coefficient is also important, especially given that we are interested in explanatory power and not causality. 

- The *R-squared*: arguably the go-to indicator for performance assessment. The total variance explained by our model (R^2) is ~ 10 percent (0.101). Low R^2 values are not uncommon, especially in the social sciences. However, when hoping to use a model for predictive purposes, 10 per cent might not be enough, the large number of statistically significant features notwithstanding. The drawback from relying solely on this measure is that it does not take into consideration the problem of model over-fitting; i.e. you can inflate the R-squared by adding as many variables as you want, even if those variables have little predicting power. This method will yield great results in the training data, but will under perform when extrapolating the model to the test (or indeed any other) data set.

*Out of sample model predictions*

Now that we have built and evaluated our model, we can proceed to make out-of-sample predictions.

```{r}
p <- predict(model1, Test_df)
```
Notice that our prediction produces a warning. The warning is just that, a warning, and it does not stop the program from running or producing results. It does, however, indicate that those results may be meaningless. This is not big news for us at this point, since we have run into some interesting insights along the way: a zero-variance predictor in the correlation matrix, a low R-squared value... 

*The bias-variance tradeoff in practice*

We previously mentioned that the RMSE metric could also be used to compare between training and test model predictions. Let us estimate the out-of-sample RMSE:

```{r}
error <- p - Test_df[["sumConsumption"]] # predicted values minus actual values
RMSE_test <- sqrt(mean(error^2))
print(RMSE_test) # this is known as the out-of-sample RMSE
```
Notice that the *in-sample RMSE*[$228.4205$] is very close to the *out-of-sample RMSE*[$232.5525$]. This means that our model makes consistent predictions across different data sets. However, we also know by now that these predictions are not great. What we are observing here is a model that has not found a balance between bias and variance. 

We can take a first glance at the realised household food consumption vs. the predicted food consumption stored in our object 'p' by selecting the first six elements of our data set using the command head()

```{r}
head(cbind(Test_df$sumConsumption,p))
```

With the first six households, we're pretty far off the mark. Another way to visualise this is to use a *Confusion Matrix* (2×2 table that shows the predicted values from the model vs. the actual values from the test data set). Remember we are trying to build a model that can accurately predict a household's food consumption needs. Assume that the government of Malawi has decided to give cash transfers to households who spend less than \$95 a week. Let's create binary variables for the realised outcome and the predicted outcome, which take on the value 1 if the household spends below \$95 a week and 0 otherwise. We can then use those binary variables to build a confusion matrix.

```{r}

Test_df$realised_consumption <- ifelse(Test_df$sumConsumption<95,1,0)
# How many households spend less than $95 a week on food?
table(Test_df$realised_consumption)
# Answer: 6114!

Test_df$predicted_consumption <- ifelse(p<95,1,0)
# How many households do we predict spend less than $95 a week?
table(Test_df$predicted_consumption)
# Answer: 3545!

# Confusion Matrix manually (it's really just a cross-tabulation!)
table(Test_df$realised_consumption,Test_df$predicted_consumption)

# Confusion Matrix from the caret package
confusionMatrix(as.factor(Test_df$realised_consumption), as.factor(Test_df$predicted_consumption)) 

```

The confusion matrix already gives us a glimpse at how a bad prediction model can affect the lives of people in need. Beyond what we can read from the cross-tabulation (or confusion matrix), the caret package confusionMatrix() function also includes some statistical measures of performance. We won't discuss all of them, but focus on the Kappa statistic, a measure of model accuracy that is adjusted by accounting for the possibility of a correct prediction by chance alone. It ranges from 0 to 1, and can be interpreted using the following thresholds:

- Poor = Less than 0.20

- Fair = 0.20 to 0.40

- Moderate = 0.40 to 0.60

- Good = 0.60 to 0.80

- Very good = 0.80 to 1.00

With a kappa value of 0.2, our model has fair (borderline poor) accuracy. Now let's recapitulate:

*What do we mean by bias in a model?*

The bias is the difference between the average prediction of our model and the true (observed) value. Minimising the bias is analogous to minimising the RMSE.  

*What do we mean by variance in a model?*

It is the observed variability of our model prediction for a given data point (how much the model can adjust given the data set). A model with high variance would yield low error values in the training data but high errors in the test data. 

Our Food Consumption prediction model therefore exhibits a low level of variance (and thus it extrapolates well to other data sets) but a high level of bias. We have created an **under-fitted model!**

**5. Challenge!**

How can we improve our model? We want to avoid under-fitting, i.e. what we did throughout this example. We also want to avoid over-fitting the model. Improve and assess your model following the example above. Here are some suggestions that you can use to improve the model:

- Rethink the predictors: remember the zero-variance predictors in the correlation matrices? Perhaps they should be removed from the model. They add nothing to our model and instead make unstable predictions. You could also revisit the original data set with more than 500 features and think of other variables that might be of interest. Finally, there were some factor features (a.k.a. categorical variables) that still had some missing values. Should we get rid of the features? Or get rid of the missing values and keep the features but decrease the sample size? This is up to you!

- Include polynomial transformations: the relationship between any given predictor and the target feature (dependent variable) might be non-linear and thus better explained by a polynomial (squared, cubic) transformation.

- Other variable transformations: remember the distribution of our consumption variable? It was highly skewed. Perhaps a log transformation might be needed? 

- Include interaction terms: perhaps the true explanatory power of a feature comes from its interaction with another feature. Say, the effect of education on consumption is dominated by the gender of the household head. We know from previous [research](https://www.povertyactionlab.org/sites/default/files/publication/Briefcase_empowering-women-through-targeted-cash-transfers_north-macedonia_10152021.pdf) that when women are recipients of cash transfers from the government, households spend more of their budget on food. 



```{r, echo=FALSE}
library(downloadthis)
download_link(
  link = "https://github.com/michelleg06/Machine-Learning-for-Public-Policy/blob/main/Malawi_regression1.R",
  button_label = "Download R script Regression 1: PMT",
  button_type = "info",
  has_icon = TRUE,
  icon = "fa fa-save",
  self_contained = FALSE
)

```

### **Challenge Solution**

Here's a smart solution to the challenge: improving the prediction ability of the ML model. It was proposed by a team of students from the UNU-MERIT MPP 2022-2023 cohort. To run this code, you need to have ran all the previous lines in the *Machine Learning with linear regression* tab. 

First, let's start by getting rid of those pesky missing values in the factor variables that we spotted earlier in our data exploration.

```{r}
# # # #
table(malawi$hh_c09)
levels(malawi$hh_c09)

malawi$hh_c09 <- droplevels(malawi$hh_c09, "")

# # # # 
table(malawi$hh_c24)
malawi$hh_c24 <- droplevels(malawi$hh_c24, "")

# # # #
table(malawi$hh_e70) # This variable has too many missings! 22,163! Perhaps it's best if we get rid of the variable instead of deleting the missing data

malawi <- malawi[,-which( colnames(malawi)=="hh_e70")] # This is a more straightforward way of deleting the undesirable variable and avoid mistakes due to typos when writing down the name of the column.

# # # #
table(malawi$hh_t08)
malawi$hh_t08 <- droplevels(malawi$hh_t08, "")

# # # #
table(malawi$hh_t01)
malawi$hh_t01 <- droplevels(malawi$hh_t01, "")

# # # #
table(malawi$hh_t14)
malawi$hh_t14 <- droplevels(malawi$hh_t14, "")

# Let's quickly check if we have any other factor variables with missing data:

malawi_factor2 <- malawi %>% select_if(~is.factor(.)) 
llply(.data=malawi_factor2, .fun=table)

```

Now that we have finally rid the entire data set from all missing values, we can proceed to improve our model with

a) Data-specific knowledge: Currently, we have a Food Consumption variable per household. However, our dataset is not at the household level, but at the user level. This means that, for each user, we have the household spending, but not the share that would go to each user. Perhaps, a better alternative would be to create a new target variable which corresponds to the users' Food Consumption, and not their household consumption.

b) Subject-specific knowledge: Usually, poverty measures look at daily spending on food, or income per day to assess whether a person has enough to cover their necessities. This [BBC article](https://www.bbc.com/news/magazine-17312819) tells us the story on how World Bank Economist Martin Ravallion came up with this measure and why it matters. 

```{r}

# What is the daily, food Consumption per capita?
malawi$Cons_pcpd <-(malawi$sumConsumption / malawi$hhsize)/7

summary(malawi$Cons_pcpd)

```

Note that the median person in Malawi lives with a little over 1 USD per day. This seems consistent with World Bank [records](https://opportunity.org/our-impact/where-we-work/malawi-facts-about-poverty). In 2017, ca. 70 per cent of Malawians were living under the international poverty line of $1.90/day$.

Now, let's proceed to run our model and assess it:

```{r eval=FALSE, warning=FALSE}
#Set a seed
set.seed(1234777)

# Split the data 

train_idx <- createDataPartition(malawi$Cons_pcpd, p = .8, list = FALSE, times = 1)

Train_df <- malawi[ train_idx,]
Test_df  <- malawi[-train_idx,]

# Run our model

model_corrected <- lm(Cons_pcpd ~ .,Train_df)
```

```{r, echo=FALSE, warning=FALSE}
stargazer(model_corrected, type = "text")

```

Note how the *R^2* has now significantly increased to $0.677$!. Let's not get our hopes up. A big driver of this increase is the fact that the variable, weekly household spending on food (our initial target variable), is used as a predictor. Notice also that there is a zero variance predictor in the model. We won't dwell on the latter and focus on the former. 

```{r, warning=FALSE}
Train_df <- Train_df[,-which(colnames(Train_df)=="sumConsumption")]
Test_df  <- Test_df[,-which(colnames(Test_df)=="sumConsumption")]

# This is a more direct way of deleting columns than what we had previously done. Just for your information, sumConsumption was column/vector number four in both dataframes, so we could have just removed column for instead of using the which() statement. 

model_corrected <- lm(Cons_pcpd ~ .,Train_df)
stargazer(model_corrected, type = "text")
```

Unfortunately, once we remove the weekly food consumption variable, from which the daily, per capita food consumption was derived, our R^2 drops down to similar levels as before: $0.105$, ca. ten per cent of the variance of our model explained.

Let's take a look at the in-sample RMSE now: 

```{r}
# 1. Predict values on the training data set
p00 <- predict(model_corrected, Train_df)

# 2. Obtain the errors (predicted values minus observed values of target variable)
error00 <- p00 - Train_df[["Cons_pcpd"]]

# 3. In-sample RMSE
print(RMSE_insample_correctedmodel <- sqrt(mean(error00 ^ 2, na.rm = T)))
```

Despite our R^2 value being low, which, again, is not uncommon in the social sciences, our in-sample RMSE is also low! We have managed to decrease the bias in the model significantly from $228.4$, all the way down to $10.00681$. Let's see if we observe a similar trend in the test data set with the out of sample RMSE.

```{r}

# 1. Predict values on the training data set
p00 <- predict(model_corrected, Test_df)

# 2. Obtain the errors (predicted values minus observed values of target variable)
error00 <- p00 - Test_df[["Cons_pcpd"]]

# 3. In-sample RMSE
print(RMSE_outofsample_correctedmodel <- sqrt(mean(error00 ^ 2, na.rm = T)))
```

With an out of sample RMSE of $10.14351$, it seems like our model is consistent across data sets. So, whilst we were not able to increase the R^2, we did significantly decrease the bias in our model. This, to me, is a success! As we learn more techniques, we will be able to build from this - already highly improved model - to try to find the most accurate prediction. 