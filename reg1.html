<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Supervised Machine Learning: Linear Models (Regression)</title>

<script src="site_libs/header-attrs-2.25/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/vembedr-0.1.5/css/vembedr.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.2/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Machine Learning for Public Policy</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
    Home
  </a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Supervised Machine Learning: Linear Models
(Regression)</h1>

</div>


<style>
    body {
    text-align: justify}
</style>
<div id="supervised-machine-learning-linear-models-regression"
class="section level2 tabset tabset-fade tabset-pills">
<h2 class="tabset tabset-fade tabset-pills"><strong>Supervised Machine
Learning: Linear Models (Regression)</strong></h2>
<p><strong>Introducing the Prediction Policy Framework</strong></p>
<p>In the video-lecture below you’ll be given a brief introduction to
the prediction policy framework, and a primer on machine learning.
Please take a moment to watch the 10 minute video.</p>
<center>
<div class="vembedr">
<div>
<iframe src="https://www.youtube.com/embed/aA7tLQxf1xE" width="533" height="300" frameborder="0" allowfullscreen="" data-external="1"></iframe>
</div>
</div>
</center>
<p>After watching the video, we have a practical excercise in R:</p>
<p><strong>Predicting social assistance beneficiaries</strong></p>
<p>Social policies usually work with tight budgets and limited fiscal
space. To allocate resources efficiently, benefits need to be targeted
to those who need them the most. Yet, identifying needs isn’t easy and
misclassifications can have severe and irreversible impacts on people in
need. Different targeting mechanisms are being used, including expert
judgment to define eligibility criteria or selection by community
committees. The most common approach for anti-poverty programs is based
on data: Proxy Means Testing (PMT). A recent review and discussion of
social assistance targeting by the World Bank can be found <a
href="https://www.worldbank.org/en/topic/socialprotection/publication/a-new-look-at-old-dilemmas-revisiting-targeting-in-social-assistance">here</a>.</p>
<center>
<div class="vembedr">
<div>
<iframe src="https://www.youtube.com/embed/Tzm9r91DBnc" width="533" height="300" frameborder="0" allowfullscreen="" data-external="1"></iframe>
</div>
</div>
</center>
<p>The underlying idea behind PMT is to use data on easily observable
household characteristics - which are hard to manipulate - to predict
poverty. Traditionally, regressions of observable characteristics on
reported consumption are performed on existing large-scale survey data
sets. The regression coefficients of the best-performing model are then
used as a weight. To define program eligibility, a second survey is
conducted with potential beneficiaries. To screen households,
information on the selected PMT variables is collected (e.g. number of
rooms, type of house, household size) and the values of the chosen
observables are multiplied by the observables’ respective weight
(derived from the previous regression). Households are then ordered
according to their predicted PMT score and benefits are allocated to
households below a certain threshold. In block 1 of this course, we use
data to build a PMT algorithm. We rely on World Bank’s LSMS household
survey data from Malawi to build a model to rank and identify households
in need. Before jumping to the data, we need to ensure that we have a
proper understanding of the problem we want to solve.</p>
<p><strong>Discussion Points</strong></p>
<ul>
<li>
Why is this a prediction policy problem? What would be a causal
inference problem in this setting? Is it a regression or a
classification problem?
</li>
<li>
Which variables and characteristics we include in the PMT can make a big
difference.
<ul>
<li>
Programmatically and conceptually, which type of characteristics do we
want to consider for the PMT?
</li>
<li>
Technically, how do we select which variables to include in a prediction
model? How is this difference from a causal inference problem?
</li>
</ul>
</li>
<li>
What are the practical implications of the bias-variance tradeoff in
this application?
</li>
<li>
What are potential risks of such a data driven targeting approach?
</li>
</ul>
<p><br></p>
<div id="merging-datasets-with-r-lsms-data-example"
class="section level3">
<h3><strong>Merging datasets with R: LSMS data example</strong></h3>
<p>Many organisations, institutions, and firms around the world collect
and, if we’re lucky, share data. One such example is The World Bank
Group who, in cooperation with local statistical offices, collects
cross-sectional and panel data on the socioeconomic lives of country
nationals. The Living Standards Measurement Survey (LSMS, for short) has
been around since the 1980’s, and it is publicly shared in various
formats and distinct files. We’ve chosen to download the 2019-2020 LSMS
data for Malawi, a cross-sectional database which contains several
modules on Health, Education, Food Security, etc. When you download the
data, you receive a folder with several .csv files that must be put
together. The code below does just that.</p>
<p><strong>1. Preliminaries: setting working directory and opening
libraries</strong></p>
<pre class="r"><code># 1.1 Cleaning the working environment and setting up the working directory path
rm(list = ls())
#setwd(&quot;~/Desktop/Malawi2019&quot;)
# 1.2 Opening libraries

#install.packages(&quot;tidyverse&quot;,&quot;data.table&quot;)
library(tidyverse)
library(data.table)</code></pre>
<p><strong>2. Uploading the data</strong></p>
<p>We will be working with the Malawi dataset from the LSMS surveys from
the World Bank Group. To download and merge the data, you can refer to
the documentation: <a
href="https://microdata.worldbank.org/index.php/catalog/3818/related-materials">World
Bank Microdata: LSMS Malawi</a>.</p>
<pre class="r"><code># The following like will look inside the folder defined by file.path() command and collect the names of all the files that contain the word patterns &quot;hh_mod&quot;. 
filedir &lt;- file.path(&quot;~/Desktop/Malawi2019&quot;)
filenames &lt;- list.files(filedir,pattern=&quot;hh_mod&quot;, full.names=TRUE, recursive=FALSE, ignore.case=TRUE)

# data object now contains all files from the 2019 round from the Household Module
print(filenames)</code></pre>
<pre><code>##  [1] &quot;/Users/michellegonzalez/Desktop/Malawi2019/hh_mod_a_filt.csv&quot;
##  [2] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_B.csv&quot;     
##  [3] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_C.csv&quot;     
##  [4] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_D.csv&quot;     
##  [5] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_E.csv&quot;     
##  [6] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_F.csv&quot;     
##  [7] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_F1.csv&quot;    
##  [8] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_G1.csv&quot;    
##  [9] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_G2.csv&quot;    
## [10] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_G3.csv&quot;    
## [11] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_H.csv&quot;     
## [12] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_I1.csv&quot;    
## [13] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_I2.csv&quot;    
## [14] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_J.csv&quot;     
## [15] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_K1.csv&quot;    
## [16] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_K2.csv&quot;    
## [17] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_L.csv&quot;     
## [18] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_M.csv&quot;     
## [19] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_META.csv&quot;  
## [20] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_N1.csv&quot;    
## [21] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_N2.csv&quot;    
## [22] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_O.csv&quot;     
## [23] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_P.csv&quot;     
## [24] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_Q.csv&quot;     
## [25] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_R.csv&quot;     
## [26] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_S1.csv&quot;    
## [27] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_S2.csv&quot;    
## [28] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_T.csv&quot;     
## [29] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_U.csv&quot;     
## [30] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_V.csv&quot;     
## [31] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_W.csv&quot;     
## [32] &quot;/Users/michellegonzalez/Desktop/Malawi2019/HH_MOD_X.csv&quot;</code></pre>
<pre class="r"><code># We will now create function that will read all the files referenced in the filenames object and assign an an object to them (each file will become an element within a list).

datList &lt;- lapply(filenames, function(i) {
    df &lt;- read.csv(i, header=TRUE, strip.white=TRUE)
    return(df)
}) # this function should read all the *listed* files from above, convert them into a data frame and store them in a list

head(datList[[1]]) # returns the first elements in the first dataframe from the list datList</code></pre>
<pre><code>##       case_id                             HHID    ea_id region district reside
## 1 1.01011e+11 7d78f2c5da59436d9bde9b09ea8a8aaf 10101100  North  Chitipa  RURAL
## 2 1.01011e+11 7144cc6d29b3485d9e6d6188b255c756 10101100  North  Chitipa  RURAL
## 3 1.01011e+11 9936d103bf974a93afbc63d477b8b3f2 10101100  North  Chitipa  RURAL
## 4 1.01011e+11 cc8f211413cd493e83e01a96aba95bbb 10101100  North  Chitipa  RURAL
## 5 1.01011e+11 e50cfa8d11b44d56891e0fad015b07c7 10101100  North  Chitipa  RURAL
## 6 1.01011e+11 4c60971913ca49b4beb6def01a0b133f 10101100  North  Chitipa  RURAL
##   interviewDate  hh_wgt hh_a02a hh_a03 hh_a06 hh_a11 hh_a13 hh_a22 hh_a23
## 1    2019-08-29 93.7194   10101    100     14     NO            14     58
## 2    2019-08-29 93.7194   10101    100     23     NO            14     64
## 3    2019-08-28 93.7194   10101    100     40     NO            14     30
## 4    2019-08-29 93.7194   10101    100     71     NO            14     30
## 5    2019-08-28 93.7194   10101    100     95     NO            14     58
## 6    2019-08-29 93.7194   10101    100    115     NO            14     70
##   hh_g09 hh_m00 hh_o0a hh_s01 hh_w01 hhsize
## 1     NO    Yes    YES    YES     NO      4
## 2    YES    Yes    YES    YES     NO      4
## 3     NO    Yes     NO     NO     NO      4
## 4     NO    Yes    YES    YES     NO      5
## 5     NO    Yes     NO    YES     NO      5
## 6    YES    Yes    YES     NO     NO      2</code></pre>
<pre class="r"><code># A smart thing to do is to clean your Global Environment every so often when you are done with an object, to free up some memory
rm(filenames, filedir)</code></pre>
<p>Note that the order of the datasets within datList follows the order
of the names in filenames, such that dataset hh_mod_a_filt.csv would be
1, HH_MOD_B.csv would be 2, and so forth.</p>
<p><strong>3. Merging data </strong></p>
<p>Now that we have succesfully uploaded all the files from our folder,
we want to merge the datasets stored in the list (datList):</p>
<ul>
<li>We want to merge the first dataset [module A], containing household
level information, with module B, a Household Roster with Individual
level information. The variable <em>case_id</em> is the unique
identifier at the household level, and it is present in both
datasets.</li>
</ul>
<pre class="r"><code>datAB &lt;- merge(datList[[1]], datList[[2]], by=&quot;case_id&quot;, all = TRUE)</code></pre>
<p>Let’s do some sanity checks to see whether our merging worked as
intended:</p>
<pre class="r"><code># number of rows and columns of module A
dim(datList[[1]]) # 11434 rows, 21 columns</code></pre>
<pre><code>## [1] 11434    21</code></pre>
<pre class="r"><code># number of rows and columns of module B
dim(datList[[2]]) # 50476 rows, 53 columns</code></pre>
<pre><code>## [1] 50476    53</code></pre>
<pre class="r"><code># number of rows and columns of our merged data. Rows MUST be the same as module B
dim(datAB) # 50476 rows # 73 columns</code></pre>
<pre><code>## [1] 50476    73</code></pre>
<pre class="r"><code># notice that the number of columns = Cols A + Cols B -1[variable we used to merge]

#Brief overview of our new dataset/dataframe!
str(datAB)</code></pre>
<pre><code>## &#39;data.frame&#39;:    50476 obs. of  73 variables:
##  $ case_id      : num  1.01e+11 1.01e+11 1.01e+11 1.01e+11 1.01e+11 ...
##  $ HHID.x       : chr  &quot;7d78f2c5da59436d9bde9b09ea8a8aaf&quot; &quot;7d78f2c5da59436d9bde9b09ea8a8aaf&quot; &quot;7d78f2c5da59436d9bde9b09ea8a8aaf&quot; &quot;7d78f2c5da59436d9bde9b09ea8a8aaf&quot; ...
##  $ ea_id        : int  10101100 10101100 10101100 10101100 10101100 10101100 10101100 10101100 10101100 10101100 ...
##  $ region       : chr  &quot;North&quot; &quot;North&quot; &quot;North&quot; &quot;North&quot; ...
##  $ district     : chr  &quot;Chitipa&quot; &quot;Chitipa&quot; &quot;Chitipa&quot; &quot;Chitipa&quot; ...
##  $ reside       : chr  &quot;RURAL&quot; &quot;RURAL&quot; &quot;RURAL&quot; &quot;RURAL&quot; ...
##  $ interviewDate: chr  &quot;2019-08-29&quot; &quot;2019-08-29&quot; &quot;2019-08-29&quot; &quot;2019-08-29&quot; ...
##  $ hh_wgt       : num  93.7 93.7 93.7 93.7 93.7 ...
##  $ hh_a02a      : int  10101 10101 10101 10101 10101 10101 10101 10101 10101 10101 ...
##  $ hh_a03       : int  100 100 100 100 100 100 100 100 100 100 ...
##  $ hh_a06       : int  14 14 14 14 23 23 23 23 40 40 ...
##  $ hh_a11       : chr  &quot;NO&quot; &quot;NO&quot; &quot;NO&quot; &quot;NO&quot; ...
##  $ hh_a13       : chr  &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_a22       : int  14 14 14 14 14 14 14 14 14 14 ...
##  $ hh_a23       : int  58 58 58 58 64 64 64 64 30 30 ...
##  $ hh_g09       : chr  &quot;NO&quot; &quot;NO&quot; &quot;NO&quot; &quot;NO&quot; ...
##  $ hh_m00       : chr  &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; ...
##  $ hh_o0a       : chr  &quot;YES&quot; &quot;YES&quot; &quot;YES&quot; &quot;YES&quot; ...
##  $ hh_s01       : chr  &quot;YES&quot; &quot;YES&quot; &quot;YES&quot; &quot;YES&quot; ...
##  $ hh_w01       : chr  &quot;NO&quot; &quot;NO&quot; &quot;NO&quot; &quot;NO&quot; ...
##  $ hhsize       : int  4 4 4 4 4 4 4 4 4 4 ...
##  $ HHID.y       : chr  &quot;7d78f2c5da59436d9bde9b09ea8a8aaf&quot; &quot;7d78f2c5da59436d9bde9b09ea8a8aaf&quot; &quot;7d78f2c5da59436d9bde9b09ea8a8aaf&quot; &quot;7d78f2c5da59436d9bde9b09ea8a8aaf&quot; ...
##  $ PID          : int  1 2 3 4 1 2 3 4 1 2 ...
##  $ hh_b03       : chr  &quot;MALE&quot; &quot;FEMALE&quot; &quot;MALE&quot; &quot;MALE&quot; ...
##  $ hh_b04       : chr  &quot;HEAD&quot; &quot;WIFE/HUSBAND&quot; &quot;CHILD/ADOPTED CHILD&quot; &quot;CHILD/ADOPTED CHILD&quot; ...
##  $ hh_b04a      : chr  &quot;NO&quot; &quot;YES&quot; &quot;NO&quot; &quot;&quot; ...
##  $ hh_b05_2     : chr  &quot;&quot; &quot;&quot; &quot;NO&quot; &quot;YES&quot; ...
##  $ hh_b05_2_1   : chr  &quot;&quot; &quot;&quot; &quot;&quot; &quot;YES&quot; ...
##  $ hh_b05a      : int  41 34 11 4 55 18 13 13 25 8 ...
##  $ hh_b05b      : int  NA NA NA 8 NA NA NA NA NA NA ...
##  $ hh_b06a      : chr  &quot;OCTOBER&quot; &quot;JANUARY&quot; &quot;NOVEMBER&quot; &quot;DECEMBER&quot; ...
##  $ hh_b06b      : int  1977 1985 2007 2014 1964 2001 2005 2005 1993 2011 ...
##  $ hh_b07       : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ hh_b08       : int  7 7 0 7 7 7 7 7 7 7 ...
##  $ hh_b09       : chr  &quot;THIS VILLAGE&quot; &quot;OTHER VILLAGE IN THIS DISTRICT&quot; &quot;THIS VILLAGE&quot; &quot;THIS VILLAGE&quot; ...
##  $ hh_b10a      : chr  &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b10b      : chr  &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b10_oth   : chr  &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b11       : chr  &quot;YES&quot; &quot;YES&quot; &quot;YES&quot; &quot;YES&quot; ...
##  $ hh_b12       : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ hh_b13       : chr  &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b13_oth   : chr  &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b14       : chr  &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b15a      : chr  &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b15b      : chr  &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b15b_oth  : chr  &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b16a      : chr  &quot;DEAD&quot; &quot;DEAD&quot; &quot;MEMBER OF HH&quot; &quot;MEMBER OF HH&quot; ...
##  $ hh_b16b      : int  NA NA 1 1 NA NA NA NA NA NA ...
##  $ hh_b17       : int  34 18 NA NA NA 7 3 3 NA NA ...
##  $ hh_b18       : chr  &quot;NONE&quot; &quot;MSCE/GCSE&quot; &quot;NONE&quot; &quot;NONE&quot; ...
##  $ hh_b19a      : chr  &quot;LIVING OUTSIDE OF HH&quot; &quot;DEAD&quot; &quot;MEMBER OF HH&quot; &quot;MEMBER OF HH&quot; ...
##  $ hh_b19b      : int  NA NA 2 2 NA 1 1 1 NA 1 ...
##  $ hh_b20       : int  NA 17 NA NA NA NA NA NA NA NA ...
##  $ hh_b21       : chr  &quot;NONE&quot; &quot;NONE&quot; &quot;NONE&quot; &quot;NONE&quot; ...
##  $ hh_b22       : chr  &quot;SUKWA/NDALI&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b22_oth   : chr  &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b23       : chr  &quot;CHRISTIANITY&quot; &quot;CHRISTIANITY&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b23_oth   : chr  &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b24       : chr  &quot;POLYGAMOUS MARRIED OR NON-FORMAL UNION&quot; &quot;MONOGAMOUS MARRIED OR NON-FORMAL UNION&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b24_1     : chr  &quot;PATRILINEAL&quot; &quot;PATRILINEAL&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b24_1_oth : chr  &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b24_2     : chr  &quot;STAY IN OWN VILLAGE AS SPOUSE IS FROM THE SAME VILLAGE&quot; &quot;STAY IN OWN VILLAGE AS SPOUSE IS FROM THE SAME VILLAGE&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b25       : chr  &quot;YES&quot; &quot;YES&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b26a      : int  2 1 NA NA NA NA NA NA NA NA ...
##  $ hh_b26a_1    : int  2006 2006 NA NA NA NA NA NA NA NA ...
##  $ hh_b26b      : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ hh_b26b_1    : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ hh_b26b_2    : chr  &quot;NO&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b26c      : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ hh_b26c_1    : int  NA NA NA NA NA NA NA NA NA NA ...
##  $ hh_b26c_2    : chr  &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b27       : chr  &quot;YES&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
##  $ hh_b28       : int  1 NA NA NA NA NA NA NA NA NA ...</code></pre>
<p>Something that you might find interesting is that the numeric
variable <em>case_id</em> (which we used to merge) looks odd. This is
due to the fact that R is reading this number in scientific form, if
you’d like to see the standard number:</p>
<pre class="r"><code>options(scipen = 100) # run this line to turn off scientific form, to set it back use options(scipen = 0)</code></pre>
<p>Now you can run the <em>str(datAB)</em> line again and see the
<em>case_id</em> variable in standard form. Let’s further merge
dataframe datAB with module C, which contains individual level data on
Education. The individual unique identifier, which exists in Module B,
and C (and now also in datAB after the merge) is <em>PID</em>:</p>
<pre class="r"><code>datABC &lt;- merge(datAB, datList[[3]], by=c(&quot;case_id&quot;,&quot;PID&quot;), all = TRUE)</code></pre>
<p>We use the household unique identifier, <em>case_id</em>, to match
households and the individual unique identifier, <em>PID</em>, to match
individuals within each household. The dataframe should increase in
column number, but not in number of observations (feel free to check
using dim() or in the global environment).# We should select other
relevant modules from the IHS5 Household Database: D[Health; case_id
PID], E[Time Use &amp; Labour; case_id PID], F[Housing, case_id], H[Food
Security, case_id], T[Subjective Assessment of Well-Being, case_id]:</p>
<pre class="r"><code>datABCD &lt;- merge(datABC, datList[[4]], by=c(&quot;case_id&quot;,&quot;PID&quot;), all = TRUE)</code></pre>
<pre><code>## Warning in merge.data.frame(datABC, datList[[4]], by = c(&quot;case_id&quot;, &quot;PID&quot;), :
## column names &#39;HHID.x&#39;, &#39;HHID.y&#39; are duplicated in the result</code></pre>
<pre class="r"><code>datABCDE &lt;- merge(datABCD, datList[[5]], by=c(&quot;case_id&quot;,&quot;PID&quot;), all = TRUE)</code></pre>
<pre><code>## Warning in merge.data.frame(datABCD, datList[[5]], by = c(&quot;case_id&quot;, &quot;PID&quot;), :
## column names &#39;HHID.x&#39;, &#39;HHID.y&#39; are duplicated in the result</code></pre>
<p>There’s a warning! It isn’t a big deal (the code still did it’s job),
but let’s try to sort it out anyway. Dataframe datABCD contains already
variables HHID.x and HHID.y. Dataframe datABCDE now contains HHID,
HHID.x and HHID.y, so perhaps it would be prudent to delete two of the
three duplicates:</p>
<pre class="r"><code># let&#39;s find out the position of these two columns in the dataframe
which( colnames(datABCDE)==&quot;HHID.x&quot;) # 3 and 74</code></pre>
<pre><code>## [1]  3 74</code></pre>
<pre class="r"><code>which( colnames(datABCDE)==&quot;HHID.y&quot;) # 23 and 129</code></pre>
<pre><code>## [1]  23 129</code></pre>
<pre class="r"><code>which( colnames(datABCDE)==&quot;HHID&quot;) # 187 (let&#39;s keep this guy)</code></pre>
<pre><code>## [1] 187</code></pre>
<pre class="r"><code>datABCDE &lt;- datABCDE[,-c(3,23,74,129)] # this line removes the four selected duplicated columns.</code></pre>
<p>Note that as we continue to merge multiple modules these duplicated
columns will reappear. This happens when two datasets containing the
same vector name <besides the unique identifier> are merged.</p>
<pre class="r"><code>#Merging Module F (Housing)
datABCDEF &lt;- merge(datABCDE, datList[[6]], by=&quot;case_id&quot;, all = TRUE)</code></pre>
<p>Up until this point, the order of datList dataframes and Modules was
the same (1-A,2-B,3-C, etc.). Now, we need to take into account that
there exist modules F1, G1, G2 etc.You can refer to Table 10: Structure
of the IHS5 Household Database of the <a
href="https://microdata.worldbank.org/index.php/catalog/2939/related-materials">Basic
Information Document</a> to confirm the correct order.</p>
<pre class="r"><code>datABCDEFH &lt;- merge(datABCDEF, datList[[11]], by=&quot;case_id&quot;, all = TRUE)
datABCDEFHT &lt;- merge(datABCDEFH, datList[[28]], by=&quot;case_id&quot;, all = TRUE)</code></pre>
<pre><code>## Warning in merge.data.frame(datABCDEFH, datList[[28]], by = &quot;case_id&quot;, all =
## TRUE): column names &#39;HHID.x&#39;, &#39;HHID.y&#39; are duplicated in the result</code></pre>
<pre class="r"><code>which( colnames(datABCDEFHT)==&quot;HHID.x&quot;) # 183 454</code></pre>
<pre><code>## [1] 183 454</code></pre>
<pre class="r"><code>which( colnames(datABCDEFHT)==&quot;HHID.y&quot;) # 340 494</code></pre>
<pre><code>## [1] 340 494</code></pre>
<pre class="r"><code>datABCDEFHT &lt;- datABCDEFHT[,-c(454,340,494)]
names(datABCDEFHT)[names(datABCDEFHT) == &quot;HHID.x&quot;] &lt;- &quot;HHID&quot; # change the name of the non-eliminated HHID.x column to HHID</code></pre>
<p>We should also include some information on consumption:</p>
<pre class="r"><code># G1[Food Consumption Over Past One Week; case_id hh_g02] 8
# This module collects information on all food consumed by the household in the past 7 days
datG1 = datList[[8]]

datG1 &lt;- datG1[,c(1,7,8)] # I only want to keep the unique household identifier, item code,  and the total amount consumed in the past week.
# We want to create a vector (variable) that contains one single number: the total amount spent on food in the past 7 days.

# Initialise empty vectors for &#39;for&#39; loop
list &lt;- unique(datG1$case_id) # list is an object containing only
idx  &lt;- NA
hold &lt;- NA
sumConsumption &lt;- NA
temp &lt;- NA
data &lt;- NA

for (i in 1:length(list)) { # iterate over the number of elements in object &#39;list&#39; 
        
        idx    &lt;- which(datG1$case_id == list[i]) #index containing the unique id that matches the iteration number
        hold   &lt;- datG1[idx,] # vector containing all the elements that match the previous indicator
        sumConsumption  &lt;- sum(hold$hh_g03a, na.rm = TRUE) # sum all the values contained in food item consumption (hh_g03a) vector (in $)
        
        temp &lt;- cbind(hold$case_id,sumConsumption) # create a temporary object with the two concatenated vectors: the unique identifier and the final consumption variable
        data  &lt;- rbind(data,temp) # append the temporary object to dataframe data
        
}

# Make sure that the object data is stored correctly: dataframe type
datG &lt;- as.data.frame(data)

# let&#39;s keep only one observation per household
datG &lt;- unique(datG)
# Now let&#39;s rename the variable and merge our consumption data!
names(datG)[names(datG) == &quot;V1&quot;] &lt;- &quot;case_id&quot;
# The first row is empty, let&#39;s get rid of it
datG &lt;- datG[-1,]

datABCDEFHTG1 &lt;- merge(datABCDEFHT, datG, by=&quot;case_id&quot;, all = TRUE)

# save/export the final, merged dataset
write_rds(datABCDEFHTG1, &quot;Malawi_2019(2).rds&quot;,&quot;xz&quot;, compression = 9L)</code></pre>
<p>You can download the final data set by clicking on the button
below.</p>
<a href="https://github.com/michelleg06/Machine-Learning-for-Public-Policy/blob/main/Malawi_2019.rds">
<button class="btn btn-info"><i class="fa fa-save"></i> Download merged file (.rds)</button>
</a>
</div>
<div id="machine-learning-with-linear-regression"
class="section level3 active">
<h3 class="active"><strong>Machine Learning with linear
regression</strong></h3>
<p>This code is a step by step on how to go about writing a predictive
model using a <em>linear regression</em>. Despite its simplicity and
transparency, i.e. the ease with which we can interpret its results, a
linear model is not without challenges in machine learning. Using the
Malawi 2019-2020 LSMS data, we will predict a known outcome, <em>Food
Consumption</em>, by training our model using correlates (i.e. features)
that would explain the phenomenon; i.e. we will build a PMT
algorithm.</p>
<p><strong>1. Preliminaries: working directory, libraries, data
upload</strong></p>
<pre class="r"><code>rm(list = ls())
#setwd(&quot;~/Desktop/MachineLearning4PP/Machine-Learning-for-Public-Policy&quot;)

# Libraries

# install.packages(&quot;caret&quot;, &quot;corrplot&quot;, &quot;Hmisc&quot;, &quot;plyr&quot;)
library(plyr)
library(tidyverse)
library(data.table)
library(caret)
library(corrplot)
library(Hmisc)
library(stargazer)

malawi &lt;- read_rds(&quot;/Users/michellegonzalez/Documents/GitHub/Machine-Learning-for-Public-Policy/Malawi_2019.rds&quot;)</code></pre>
<p><strong>2. Data Pre-Processing: data visualisation and data
wrangling</strong></p>
<p>The Malawi dataset contains 50,476 observations and 514 features
(variables). It is a very large dataframe, and we want to work with only
a subset of it using all the observations but only some features;
i.e. we’re going to create a subset of the dataframe that contains only
the relevant features for our PMT.You can find the data label
descriptions <a
href="https://microdata.worldbank.org/index.php/catalog/3818/data-dictionary">here</a>.</p>
<pre class="r"><code>dim(malawi)</code></pre>
<pre><code>## [1] 50476   514</code></pre>
<p>We should begin the <em>critical</em> process of feature selection
(or variable selection). Some notes on this: from a practical point of
view, a model with less predictors may be easier to interpret. Also,
some models may be negatively affected by non-informative predictors.
This process is similar to traditional econometric modeling, but we
should not conflate predictive and explanatory modeling. Importantly,
please note that we are not interested in knowing why something happens,
but rather in what is likely to happen given some known data. Hence:</p>
<p><em>Target variable</em> (a.k.a. Dependent Variable): Total Food
Consumption $ in the Past 7 Days</p>
<p>Which variables would help us predict Malawian households’ weekly
food-spending?</p>
<pre class="r"><code># By looking at the data dictionary, I&#39;ve selected the following variables:

#1 $ food consumption in the past week: sumConsumption
which( colnames(malawi)==&quot;sumConsumption&quot;) # 514
#2 How old is NAME (year): hh_b05a
which( colnames(malawi)==&quot;hh_b05a&quot;) # 27
#3 reside: reside
which( colnames(malawi)==&quot;reside&quot;) # 6
#4 household size: hhsize
which( colnames(malawi)==&quot;hhsize&quot;) #21 
#5 did you or anyone else in this HH borrow on credit?: hh_s01
which( colnames(malawi)==&quot;hh_s01&quot;) # 19
#6 gender [1 MALE, 2 FEMALE]: hh_b03 
which( colnames(malawi)==&quot;hh_b03&quot;) # 22
#7 What is the highest educational qualification [NAME] has acquired: hh_c09
which( colnames(malawi)==&quot;hh_c09&quot;) # 88
#8 $ Tuition, including any extra tuition fees (hh_c22a)
which( colnames(malawi)==&quot;hh_c22a&quot;) # 105
#9 $ Expenditures on after school programs &amp; tutoring (hh_c22b)
which( colnames(malawi)==&quot;hh_c22b&quot;) # 106
#10 $ School books and other materials (hh_c22c)
which( colnames(malawi)==&quot;hh_c22c&quot;) # 107
#11 $ School uniform clothing (hh_c22d)
which( colnames(malawi)==&quot;hh_c22d&quot;) #108
#12 $ TOTAL (hh_c22j)
which( colnames(malawi)==&quot;hh_c22j&quot;) #114
#13 How much was spent in TOTAL in the last 12 months? (hh_c22l)
which( colnames(malawi)==&quot;hh_c22l&quot;) #116
#14 Have you used a computer? (hh_c24)
which( colnames(malawi)==&quot;hh_c24&quot;) # 124
#15 Amt [NAME] spent in the past 4 weeks for all illnesses and injuries? (hh_d10)
which( colnames(malawi)==&quot;hh_d10&quot;) # 142
#16 Amt in total did [NAME]spent...for medical care not related to an illness? (hh_d11)
which( colnames(malawi)==&quot;hh_d11&quot;) #143 
#17 How much in total did [NAME] spend..for medical insurance? (hh_d12_1)
which( colnames(malawi)==&quot;hh_d12_1&quot;) #145
#18 Does [NAME] want to change his/her current employment situation? (hh_e70)
which( colnames(malawi)==&quot;hh_e70&quot;) #337
#19 What was the total amout paid in the form of land rent during the p... (hh_f04_4)
which( colnames(malawi)==&quot;hh_f04_4&quot;) #355
#20 hh_f04a How much do you pay to rent this property?
which( colnames(malawi)==&quot;hh_f04a&quot;) #358
#21 Do you have electricity working in your dwelling? (hh_f19)
which( colnames(malawi)==&quot;hh_f19&quot;) #381
#22 How many working cell phones in total does your household own? (hh_f34)
which( colnames(malawi)==&quot;hh_f34&quot;) #404
#23 Which of the following is true? Your current income . . . (hh_t08)
which( colnames(malawi)==&quot;hh_t08&quot;) #499
#24 Concerning your HH&#39;s food consumption, over the past months which is true (hh_t01)
which( colnames(malawi)==&quot;hh_t01&quot;) #492
#25 HH were unable to eat healthy &amp; nutritious food b&#39;se of a lack of money/other (hh_t14)
which( colnames(malawi)==&quot;hh_t14&quot;) #507
#26 ... What other features do you consider important?

# subsetting our dataframe
cols   &lt;- c(6,21,27,514,19,22,88,105,106,107,108,114,116,124,142,143,145,337,355,358,381,404,499,492,507)
malawi &lt;- malawi[,cols] 
# The malawi dataframe should only have 25 (selected) features now
# Also know that you can subset with the column name instead of the number of the column. In this exercise we use the number as a way to introduce the which() function, which may come in handy in the future. </code></pre>
<p><em>Missing values</em></p>
<p>In traditional econometric models, we often discuss what we can do in
the presence of missing data. First and foremost, we should assess
whether there is a pattern to missingness and if so, what that means to
what we can learn from our (sub)population. This remains true for
machine learning. A practical difference is that while one can implement
a causal econometric model with missing values, many machine learning
models fail if there are any in the training or control data.</p>
<pre class="r"><code># Missing values (NAs) by feature
colSums(is.na(malawi))</code></pre>
<pre><code>##         reside         hhsize        hh_b05a sumConsumption         hh_s01 
##              0              0              0           9046              0 
##         hh_b03         hh_c09        hh_c22a        hh_c22b        hh_c22c 
##              0              0          32402          32398          32408 
##        hh_c22d        hh_c22j        hh_c22l         hh_c24         hh_d10 
##          32402          32397          32542              0              0 
##         hh_d11       hh_d12_1         hh_e70       hh_f04_4        hh_f04a 
##              0              0              0          50121          44487 
##         hh_f19         hh_f34         hh_t08         hh_t01         hh_t14 
##              0              0              0              0              0</code></pre>
<p>Interestingly, a lot of the features from module C have more than
half the values missing (&gt;30,000 hhs). All of the variables refer to
$ spent on education. For the sake of simplicity, I will simply remove
those features. But this action may later affect our predictive model,
if such features were to increase its predictive performance. Another
alternative would be to only work with the remaining households for
which we have full data. However, that would reduce our sample size
significantly and I would discourage it. Finally, multiple imputation is
not feasible due to the <a
href="https://gist.github.com/farrajota/a733524a814596b0124d068a55221c29">large
share of missingness in the data</a>.</p>
<pre class="r"><code>#hh_c22a-l
which( colnames(malawi)==&quot;hh_c22a&quot;) # 8</code></pre>
<pre><code>## [1] 8</code></pre>
<pre class="r"><code>which( colnames(malawi)==&quot;hh_c22b&quot;) # 9</code></pre>
<pre><code>## [1] 9</code></pre>
<pre class="r"><code>which( colnames(malawi)==&quot;hh_c22c&quot;) # 10</code></pre>
<pre><code>## [1] 10</code></pre>
<pre class="r"><code>which( colnames(malawi)==&quot;hh_c22d&quot;) #11</code></pre>
<pre><code>## [1] 11</code></pre>
<pre class="r"><code>which( colnames(malawi)==&quot;hh_c22j&quot;) #12</code></pre>
<pre><code>## [1] 12</code></pre>
<pre class="r"><code>which( colnames(malawi)==&quot;hh_c22l&quot;) #13</code></pre>
<pre><code>## [1] 13</code></pre>
<pre class="r"><code>cols_cModule &lt;- c(8,9,10,11,12,13)
malawi &lt;- malawi[,-cols_cModule]
# we have 19 features left in the malawi dataframe

# Are there any remaining missing values? double-checking is always a good idea!
colSums(is.na(malawi))</code></pre>
<pre><code>##         reside         hhsize        hh_b05a sumConsumption         hh_s01 
##              0              0              0           9046              0 
##         hh_b03         hh_c09         hh_c24         hh_d10         hh_d11 
##              0              0              0              0              0 
##       hh_d12_1         hh_e70       hh_f04_4        hh_f04a         hh_f19 
##              0              0          50121          44487              0 
##         hh_f34         hh_t08         hh_t01         hh_t14 
##              0              0              0              0</code></pre>
<pre class="r"><code># There seem to be quite a lot of missing values in a couple of vectors from the F module
which( colnames(malawi)==&quot;hh_f04_4&quot;) # 13</code></pre>
<pre><code>## [1] 13</code></pre>
<pre class="r"><code>which( colnames(malawi)==&quot;hh_f04a&quot;) # 14</code></pre>
<pre><code>## [1] 14</code></pre>
<pre class="r"><code>malawi &lt;- malawi[,-c(13,14)]

# The target feature, Food Consumption in the past 7 days [sumConsumption], has 9046 missing values. Let&#39;s get rid of them
rows_consumption &lt;- which(is.na(malawi$sumConsumption))
head(rows_consumption)</code></pre>
<pre><code>## [1] 41431 41432 41433 41434 41435 41436</code></pre>
<pre class="r"><code>tail(rows_consumption)</code></pre>
<pre><code>## [1] 50471 50472 50473 50474 50475 50476</code></pre>
<pre class="r"><code>length(rows_consumption) # we&#39;ve created an indicator with all the missing rows</code></pre>
<pre><code>## [1] 9046</code></pre>
<pre class="r"><code>malawi &lt;- malawi[-rows_consumption,]
colSums(is.na(malawi)) # all clear!</code></pre>
<pre><code>##         reside         hhsize        hh_b05a sumConsumption         hh_s01 
##              0              0              0              0              0 
##         hh_b03         hh_c09         hh_c24         hh_d10         hh_d11 
##              0              0              0              0              0 
##       hh_d12_1         hh_e70         hh_f19         hh_f34         hh_t08 
##              0              0              0              0              0 
##         hh_t01         hh_t14 
##              0              0</code></pre>
<p><em>Visualising our data</em></p>
<p>A quick and effective way to take a first glance at our data is to
plot <strong>histograms</strong> of relevant (continuously distributed)
features.</p>
<pre class="r"><code>malawi_continuous &lt;- malawi %&gt;% select_if(~is.integer(.) | is.numeric(.)) # this line selects all variables in the dataframe which are integer OR numeric, and can therefore be plotted as a histogram.
hist.data.frame(malawi_continuous) # from the Hmisc package, quick and painless.</code></pre>
<p><img src="reg1_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Some important information we can gather from these plots: our food
consumption measure is skewed (perhaps it should be logged?), there are
a number of variables for which there seems to be bunching at the zero
value. The only (close to) normal distribution we observe is in terms of
household size.</p>
<p>We should plot the remaining factor (binary, categorical)
features.</p>
<pre class="r"><code>malawi_factor &lt;- malawi %&gt;% select_if(~is.factor(.)) # subset of the dataframe containing only factor variables
llply(.data=malawi_factor, .fun=table) # create tables of all the variables in dataframe using the plyr package</code></pre>
<pre><code>## $reside
## 
## RURAL URBAN 
## 35814  5616 
## 
## $hh_s01
## 
##    NO   YES 
## 28946 12484 
## 
## $hh_b03
## 
## FEMALE   MALE 
##  21501  19929 
## 
## $hh_c09
## 
##             A-LEVEL    DEGREE   DIPLOMA       JCE   MASTERS MSCE/GCSE      NONE 
##     10012       115       104       263      1794        23      1677     24110 
##       PhD      PSLC 
##         4      3328 
## 
## $hh_c24
## 
##          No   Yes 
##  5655 34279  1496 
## 
## $hh_e70
## 
##          No   Yes 
## 22163 15720  3547 
## 
## $hh_f19
## 
##    NO   YES 
## 36824  4606 
## 
## $hh_t08
## 
##                                                                     
##                                                                   3 
##                                    ALLOWS YOU TO BUILD YOUR SAVINGS 
##                                                                2443 
##                                    ALLOWS YOU TO SAVE JUST A LITTLE 
##                                                                5762 
## IS NOT SUFFICIENT, SO YOU NEED TO USE YOUR SAVINGS TO MEET EXPENSES 
##                                                                8147 
##    IS REALLY NOT SUFFICIENT, SO YOU NEED TO BORROW TO MEET EXPENSES 
##                                                                7760 
##                                       ONLY JUST MEETS YOUR EXPENSES 
##                                                               17315 
## 
## $hh_t01
## 
##                                               
##                                             3 
##      It was just adequate for household needs 
##                                         13520 
## It was less than adequate for household needs 
##                                         25794 
## It was more than adequate for household needs 
##                                          2113 
## 
## $hh_t14
## 
##            DON&#39;T KNOW         NO        YES 
##          3         25       7949      33453</code></pre>
<p>Some of what we can gather from looking at the tables is that there
are features for which the levels (also known as categories) are not
labeled in a way that we can understand. For example, feature hh_c09
(What is the highest educational qualification [NAME] has acquired) has
about 10,012 values under an unnamed educational qualification. Two
things may be going on here: 1) the enumerators coded missing values
differently from NA (which R automatically reads as missing), or 2)
there is a value which was unlabeled and we may need to relabel.</p>
<pre class="r"><code>levels(malawi$hh_c09)</code></pre>
<pre><code>##  [1] &quot;&quot;          &quot;A-LEVEL&quot;   &quot;DEGREE&quot;    &quot;DIPLOMA&quot;   &quot;JCE&quot;       &quot;MASTERS&quot;  
##  [7] &quot;MSCE/GCSE&quot; &quot;NONE&quot;      &quot;PhD&quot;       &quot;PSLC&quot;</code></pre>
<pre class="r"><code># the line above tells us that there are empty cells, recognised by the level &quot;&quot;.
head(malawi$hh_c09) # we can confirm that it is indeed an empty cell by looking at the first six values of the dataframe (using the head command, which returns the first six elements). One of the six elements is nothing. </code></pre>
<pre><code>## [1] NONE NONE NONE      NONE NONE
## Levels:  A-LEVEL DEGREE DIPLOMA JCE MASTERS MSCE/GCSE NONE PhD PSLC</code></pre>
<p>The main take away from creating tables of all the factor variables
in the dataframe is that there are still missing values that were not
taken care of during the data cleaning step. This is due to the fact
that, as R read the factors, it mistakenly read an empty cell as a
category, and not a missing value.</p>
<p><em>Exploring relationships between features</em></p>
<p>Let’s explore the relationships that may or may not exist between our
selected features using two distinct correlation matrices.</p>
<pre class="r"><code>M &lt;- cor(malawi_continuous) # create a correlation matrix of the continuous dataset, cor() uses Pearson&#39;s correlation coefficient as default. This means we can only take the correlation between continuous variables
corrplot(M, method=&quot;circle&quot;, addCoef.col =&quot;black&quot;, number.cex = 0.8) # visualise it in a nice way

# Let&#39;s compute the Spearman correlation coefficient between categorical variables
malawi_factorC &lt;- as.data.frame(lapply(malawi_factor,as.numeric)) # coerce dataframe to numeric, as the cor() command only takes in numeric types

# add the target variable to the factor correlation matrix
malawi_factorC &lt;- cbind.data.frame(malawi_factorC,malawi_continuous$sumConsumption)
names(malawi_factorC)[names(malawi_factorC) == &quot;malawi_continuous$sumConsumption&quot;] &lt;- &quot;sumConsumption&quot;

M2 &lt;- cor(malawi_factorC, method = &quot;spearman&quot;)
corrplot(M2, method=&quot;circle&quot;, addCoef.col =&quot;black&quot;, number.cex = 0.7) # visualise it in a nice way</code></pre>
<p><img src="reg1_files/figure-html/figures-side-1.png" width="50%" /><img src="reg1_files/figure-html/figures-side-2.png" width="50%" /></p>
<p>Notice how, in both matrices, but particularly that which contains
our target variable, we observe several instances were the correlation
coefficient is zero. We call this, a zero-variance predictor. For many
machine learning models (excluding tree-based models), this may cause
the model to crash or the fit to be unstable.</p>
<p><strong>3. Partition data into training and test
datasets</strong></p>
<p>When we are working with machine learning learning models, it is
important to have two data sets. A training data set from which our
model will learn, and a test data set containing the same features as
our training data set. To split our main data set into two, we will work
with an 80/20 split.</p>
<p>The 80/20 split has its origins in the Pareto Principle, which states
that “in most cases, 80% of effects from from 20% of causes”. Without
other relevant knowledge of the source or shape of the data, this
partitioning method is a good place to start and indeed standard in the
machine learning field.</p>
<pre class="r"><code># Always set a seet for reproducibility purposes!
set.seed(1234777) # use any number you want

# We could split the data manually, but the caret package includes an useful function

train_idx &lt;- createDataPartition(malawi$sumConsumption, p = .8, list = FALSE, times = 1)
head(train_idx)</code></pre>
<pre><code>##      Resample1
## [1,]         2
## [2,]         3
## [3,]         4
## [4,]         5
## [5,]         6
## [6,]         8</code></pre>
<pre class="r"><code>Train_df &lt;- malawi[ train_idx,]
Test_df  &lt;- malawi[-train_idx,]</code></pre>
<p><strong>4. Creating our first predictive model and evaluating its
performance: RMSE and R^2</strong></p>
<p>We will start by fitting a predictive model using the training
dataset; that is, our target variable <em>Total household food
consumption in the past 7 days</em> or <em>sumConsumption</em> will be a
<span class="math inline">\(Y\)</span> dependent variable in a linear
model <span class="math inline">\(Y_i = \alpha + x&#39;\beta_i +
\epsilon_i\)</span>, and the remaining features in the data frame
correspond to the row vectors <span
class="math inline">\(x&#39;\beta\)</span>.</p>
<pre class="r"><code>model1 &lt;- lm(sumConsumption ~ .,Train_df) # the dot after the squiggle ~ asks the lm() function tu use all other variables in the dataframe as predictors to the dependent variable sumConsumption
stargazer(model1, type = &quot;text&quot;)</code></pre>
<pre><code>## 
## =====================================================================================================
##                                                                               Dependent variable:    
##                                                                           ---------------------------
##                                                                                 sumConsumption       
## -----------------------------------------------------------------------------------------------------
## resideURBAN                                                                        13.259***         
##                                                                                     (4.236)          
##                                                                                                      
## hhsize                                                                             1.683***          
##                                                                                     (0.585)          
##                                                                                                      
## hh_b05a                                                                             -0.157*          
##                                                                                     (0.084)          
##                                                                                                      
## hh_s01YES                                                                           4.692*           
##                                                                                     (2.778)          
##                                                                                                      
## hh_b03MALE                                                                           3.371           
##                                                                                     (2.534)          
##                                                                                                      
## hh_c09A-LEVEL                                                                       -35.335          
##                                                                                    (24.360)          
##                                                                                                      
## hh_c09DEGREE                                                                        -23.290          
##                                                                                    (26.391)          
##                                                                                                      
## hh_c09DIPLOMA                                                                      40.064**          
##                                                                                    (17.755)          
##                                                                                                      
## hh_c09JCE                                                                          23.005***         
##                                                                                     (7.422)          
##                                                                                                      
## hh_c09MASTERS                                                                        4.132           
##                                                                                    (56.879)          
##                                                                                                      
## hh_c09MSCE/GCSE                                                                    21.789***         
##                                                                                     (8.056)          
##                                                                                                      
## hh_c09NONE                                                                          9.961**          
##                                                                                     (4.370)          
##                                                                                                      
## hh_c09PhD                                                                           -78.808          
##                                                                                    (114.431)         
##                                                                                                      
## hh_c09PSLC                                                                         15.682**          
##                                                                                     (6.117)          
##                                                                                                      
## hh_c24No                                                                          -17.959***         
##                                                                                     (5.749)          
##                                                                                                      
## hh_c24Yes                                                                          72.108***         
##                                                                                     (9.900)          
##                                                                                                      
## hh_d10                                                                              0.002**          
##                                                                                     (0.001)          
##                                                                                                      
## hh_d11                                                                              -0.001           
##                                                                                     (0.001)          
##                                                                                                      
## hh_d12_1                                                                            0.0001           
##                                                                                    (0.00005)         
##                                                                                                      
## hh_e70No                                                                            7.392**          
##                                                                                     (3.099)          
##                                                                                                      
## hh_e70Yes                                                                          15.542***         
##                                                                                     (4.994)          
##                                                                                                      
## hh_f19YES                                                                          68.338***         
##                                                                                     (4.953)          
##                                                                                                      
## hh_f34                                                                             25.370***         
##                                                                                     (1.473)          
##                                                                                                      
## hh_t08ALLOWS YOU TO BUILD YOUR SAVINGS                                              181.985          
##                                                                                    (161.857)         
##                                                                                                      
## hh_t08ALLOWS YOU TO SAVE JUST A LITTLE                                              148.168          
##                                                                                    (161.815)         
##                                                                                                      
## hh_t08IS NOT SUFFICIENT, SO YOU NEED TO USE YOUR SAVINGS TO MEET EXPENSES           117.798          
##                                                                                    (161.814)         
##                                                                                                      
## hh_t08IS REALLY NOT SUFFICIENT, SO YOU NEED TO BORROW TO MEET EXPENSES              126.248          
##                                                                                    (161.814)         
##                                                                                                      
## hh_t08ONLY JUST MEETS YOUR EXPENSES                                                 127.308          
##                                                                                    (161.796)         
##                                                                                                      
## hh_t01It was just adequate for household needs                                    -64.656***         
##                                                                                     (6.097)          
##                                                                                                      
## hh_t01It was less than adequate for household needs                               -75.411***         
##                                                                                     (6.262)          
##                                                                                                      
## hh_t01It was more than adequate for household needs                                                  
##                                                                                                      
##                                                                                                      
## hh_t14DON&#39;T KNOW                                                                  253.181***         
##                                                                                    (53.908)          
##                                                                                                      
## hh_t14NO                                                                           40.079***         
##                                                                                     (3.775)          
##                                                                                                      
## hh_t14YES                                                                                            
##                                                                                                      
##                                                                                                      
## Constant                                                                            15.806           
##                                                                                    (161.744)         
##                                                                                                      
## -----------------------------------------------------------------------------------------------------
## Observations                                                                        33,146           
## R2                                                                                   0.101           
## Adjusted R2                                                                          0.100           
## Residual Std. Error                                                          228.534 (df = 33113)    
## F Statistic                                                               116.660*** (df = 32; 33113)
## =====================================================================================================
## Note:                                                                     *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<p>The output of our model, obtained with the summary() command, or
stargazer(), has three important indicators to assess the performance of
our model:</p>
<ul>
<li>Model <em>residuals</em>: recall residuals are the observed value
minus the predicted value.</li>
</ul>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -508.94  -76.74  -44.06    0.00  -12.11 3923.07</code></pre>
<p>The max(imum) error of <span class="math inline">\(3923.07\)</span>
suggests that the model under-predicted expenses by circa $4,000 for at
least one observation. Fifty percent of the predictions (between the
first and third quartiles) over-predict the true consumption value by
$76.74 and $12.11. From these data, we obtain the popular measure of
performance evaluation known as the Root Mean Squared Error (RMSE, for
short).</p>
<pre class="r"><code># Calculate the RMSE for the training dataset, or the in-sample RMSE.

# 1. Predict values on the training dataset
p0 &lt;- predict(model1, Train_df)

# 2. Obtain the errors (predicted values minus observed values of target variable)
error0 &lt;- p0 - Train_df[[&quot;sumConsumption&quot;]]

# 3. In-sample RMSE
RMSE_insample &lt;- sqrt(mean(error0 ^ 2))
print(RMSE_insample)</code></pre>
<pre><code>## [1] 228.4205</code></pre>
<p>The RMSE (<span class="math inline">\(228.4205\)</span>) gives us an
absolute number that indicates how much our predicted values deviate
from the true (observed) number. Think of the question, <em>how far, on
average, are the residuals away from zero?</em> Generally speaking, the
lower the value, the better the model fit. Besides being a good measure
of goodness of fit, the RMSE is also useful for comparing the ability of
our model to make predictions on different (e.g. test) data sets. The
in-sample RMSE should be close or equal to the out-of-sample RMSE.</p>
<ul>
<li><p>The <em>p-values</em>: represented by stars *** indicate the
predictive power of each feature in the model. In the same line, the
magnitude of the coefficient is also important, especially given that we
are interested in explanatory power and not causality.</p></li>
<li><p>The <em>R-squared</em>: arguably the go-to indicator for
performance assessment. The total variance explained by our model (R^2)
is ~ 10 percent (0.101). Low R^2 values are not uncommon, especially in
the social sciences. However, when hoping to use a model for predictive
purposes, 10 per cent might not be enough, the large number of
statistically significant features notwithstanding. The drawback from
relying solely on this measure is that it does not take into
consideration the problem of model over-fitting; i.e. you can inflate
the R-squared by adding as many variables as you want, even if those
variables have little predicting power. This method will yield great
results in the training data, but will under perform when extrapolating
the model to the test (or indeed any other) data set.</p></li>
</ul>
<p><em>Out of sample model predictions</em></p>
<p>Now that we have built and evaluated our model, we can proceed to
make out-of-sample predictions.</p>
<pre class="r"><code>p &lt;- predict(model1, Test_df)</code></pre>
<pre><code>## Warning in predict.lm(model1, Test_df): prediction from a rank-deficient fit
## may be misleading</code></pre>
<p>Notice that our prediction produces a warning. The warning is just
that, a warning, and it does not stop the program from running or
producing results. It does, however, indicate that those results may be
meaningless. This is not big news for us at this point, since we have
run into some interesting insights along the way: a zero-variance
predictor in the correlation matrix, a low R-squared value…</p>
<p><em>The bias-variance tradeoff in practice</em></p>
<p>We previously mentioned that the RMSE metric could also be used to
compare between training and test model predictions. Let us estimate the
out-of-sample RMSE:</p>
<pre class="r"><code>error &lt;- p - Test_df[[&quot;sumConsumption&quot;]] # predicted values minus actual values
RMSE_test &lt;- sqrt(mean(error^2))
print(RMSE_test) # this is known as the out-of-sample RMSE</code></pre>
<pre><code>## [1] 232.5525</code></pre>
<p>Notice that the <em>in-sample RMSE</em>[<span
class="math inline">\(228.4205\)</span>] is very close to the
<em>out-of-sample RMSE</em>[<span
class="math inline">\(232.5525\)</span>]. This means that our model
makes consistent predictions across different data sets. However, we
also know by now that these predictions are not great. What we are
observing here is a model that has not found a balance between bias and
variance.</p>
<p>We can take a first glance at the realised household food consumption
vs. the predicted food consumption stored in our object ‘p’ by selecting
the first six elements of our data set using the command head()</p>
<pre class="r"><code>head(cbind(Test_df$sumConsumption,p))</code></pre>
<pre><code>##                  p
## 1  71.05 139.47176
## 7  37.50  89.82853
## 23 51.50  45.20280
## 27 42.00 195.02980
## 31 74.50 131.10278
## 32 74.50 134.63020</code></pre>
<p>With the first six households, we’re pretty far off the mark. Another
way to visualise this is to use a <em>Confusion Matrix</em> (2×2 table
that shows the predicted values from the model vs. the actual values
from the test data set). Remember we are trying to build a model that
can accurately predict a household’s food consumption needs. Assume that
the government of Malawi has decided to give cash transfers to
households who spend less than $95 a week. Let’s create binary variables
for the realised outcome and the predicted outcome, which take on the
value 1 if the household spends below $95 a week and 0 otherwise. We can
then use those binary variables to build a confusion matrix.</p>
<pre class="r"><code>Test_df$realised_consumption &lt;- ifelse(Test_df$sumConsumption&lt;95,1,0)
# How many households spend less than $95 a week on food?
table(Test_df$realised_consumption)</code></pre>
<pre><code>## 
##    0    1 
## 2170 6114</code></pre>
<pre class="r"><code># Answer: 6114!

Test_df$predicted_consumption &lt;- ifelse(p&lt;95,1,0)
# How many households do we predict spend less than $95 a week?
table(Test_df$predicted_consumption)</code></pre>
<pre><code>## 
##    0    1 
## 4739 3545</code></pre>
<pre class="r"><code># Answer: 3545!

# Confusion Matrix manually (it&#39;s really just a cross-tabulation!)
table(Test_df$realised_consumption,Test_df$predicted_consumption)</code></pre>
<pre><code>##    
##        0    1
##   0 1684  486
##   1 3055 3059</code></pre>
<pre class="r"><code># Confusion Matrix from the caret package
confusionMatrix(as.factor(Test_df$realised_consumption), as.factor(Test_df$predicted_consumption)) </code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 1684  486
##          1 3055 3059
##                                              
##                Accuracy : 0.5725             
##                  95% CI : (0.5618, 0.5832)   
##     No Information Rate : 0.5721             
##     P-Value [Acc &gt; NIR] : 0.4692             
##                                              
##                   Kappa : 0.2                
##                                              
##  Mcnemar&#39;s Test P-Value : &lt;0.0000000000000002
##                                              
##             Sensitivity : 0.3553             
##             Specificity : 0.8629             
##          Pos Pred Value : 0.7760             
##          Neg Pred Value : 0.5003             
##              Prevalence : 0.5721             
##          Detection Rate : 0.2033             
##    Detection Prevalence : 0.2620             
##       Balanced Accuracy : 0.6091             
##                                              
##        &#39;Positive&#39; Class : 0                  
## </code></pre>
<p>The confusion matrix already gives us a glimpse at how a bad
prediction model can affect the lives of people in need. Beyond what we
can read from the cross-tabulation (or confusion matrix), the caret
package confusionMatrix() function also includes some statistical
measures of performance. We won’t discuss all of them, but focus on the
Kappa statistic, a measure of model accuracy that is adjusted by
accounting for the possibility of a correct prediction by chance alone.
It ranges from 0 to 1, and can be interpreted using the following
thresholds:</p>
<ul>
<li><p>Poor = Less than 0.20</p></li>
<li><p>Fair = 0.20 to 0.40</p></li>
<li><p>Moderate = 0.40 to 0.60</p></li>
<li><p>Good = 0.60 to 0.80</p></li>
<li><p>Very good = 0.80 to 1.00</p></li>
</ul>
<p>With a kappa value of 0.2, our model has fair (borderline poor)
accuracy. Now let’s recapitulate:</p>
<p><em>What do we mean by bias in a model?</em></p>
<p>The bias is the difference between the average prediction of our
model and the true (observed) value. Minimising the bias is analogous to
minimising the RMSE.</p>
<p><em>What do we mean by variance in a model?</em></p>
<p>It is the observed variability of our model prediction for a given
data point (how much the model can adjust given the data set). A model
with high variance would yield low error values in the training data but
high errors in the test data.</p>
<p>Our Food Consumption prediction model therefore exhibits a low level
of variance (and thus it extrapolates well to other data sets) but a
high level of bias. We have created an <strong>under-fitted
model!</strong></p>
<p><strong>5. Challenge!</strong></p>
<p>How can we improve our model? We want to avoid under-fitting,
i.e. what we did throughout this example. We also want to avoid
over-fitting the model. Improve and assess your model following the
example above. Here are some suggestions that you can use to improve the
model:</p>
<ul>
<li><p>Rethink the predictors: remember the zero-variance predictors in
the correlation matrices? Perhaps they should be removed from the model.
They add nothing to our model and instead make unstable predictions. You
could also revisit the original data set with more than 500 features and
think of other variables that might be of interest. Finally, there were
some factor features (a.k.a. categorical variables) that still had some
missing values. Should we get rid of the features? Or get rid of the
missing values and keep the features but decrease the sample size? This
is up to you!</p></li>
<li><p>Include polynomial transformations: the relationship between any
given predictor and the target feature (dependent variable) might be
non-linear and thus better explained by a polynomial (squared, cubic)
transformation.</p></li>
<li><p>Other variable transformations: remember the distribution of our
consumption variable? It was highly skewed. Perhaps a log transformation
might be needed?</p></li>
<li><p>Include interaction terms: perhaps the true explanatory power of
a feature comes from its interaction with another feature. Say, the
effect of education on consumption is dominated by the gender of the
household head. We know from previous <a
href="https://www.povertyactionlab.org/sites/default/files/publication/Briefcase_empowering-women-through-targeted-cash-transfers_north-macedonia_10152021.pdf">research</a>
that when women are recipients of cash transfers from the government,
households spend more of their budget on food.</p></li>
</ul>
<a href="https://github.com/michelleg06/Machine-Learning-for-Public-Policy/blob/main/Malawi_regression1.R">
<button class="btn btn-info"><i class="fa fa-save"></i> Download R script Regression 1: PMT</button>
</a>
</div>
<div id="challenge-solution" class="section level3">
<h3><strong>Challenge Solution</strong></h3>
<p>Here’s a smart solution to the challenge: improving the prediction
ability of the ML model. It was proposed by a team of students from the
UNU-MERIT MPP 2022-2023 cohort. To run this code, you need to have ran
all the previous lines in the <em>Machine Learning with linear
regression</em> tab.</p>
<p>First, let’s start by getting rid of those pesky missing values in
the factor variables that we spotted earlier in our data
exploration.</p>
<pre class="r"><code># # # #
table(malawi$hh_c09)</code></pre>
<pre><code>## 
##             A-LEVEL    DEGREE   DIPLOMA       JCE   MASTERS MSCE/GCSE      NONE 
##     10012       115       104       263      1794        23      1677     24110 
##       PhD      PSLC 
##         4      3328</code></pre>
<pre class="r"><code>levels(malawi$hh_c09)</code></pre>
<pre><code>##  [1] &quot;&quot;          &quot;A-LEVEL&quot;   &quot;DEGREE&quot;    &quot;DIPLOMA&quot;   &quot;JCE&quot;       &quot;MASTERS&quot;  
##  [7] &quot;MSCE/GCSE&quot; &quot;NONE&quot;      &quot;PhD&quot;       &quot;PSLC&quot;</code></pre>
<pre class="r"><code>malawi$hh_c09 &lt;- droplevels(malawi$hh_c09, &quot;&quot;)

# # # # 
table(malawi$hh_c24)</code></pre>
<pre><code>## 
##          No   Yes 
##  5655 34279  1496</code></pre>
<pre class="r"><code>malawi$hh_c24 &lt;- droplevels(malawi$hh_c24, &quot;&quot;)

# # # #
table(malawi$hh_e70) # This variable has too many missings! 22,163! Perhaps it&#39;s best if we get rid of the variable instead of deleting the missing data</code></pre>
<pre><code>## 
##          No   Yes 
## 22163 15720  3547</code></pre>
<pre class="r"><code>malawi &lt;- malawi[,-which( colnames(malawi)==&quot;hh_e70&quot;)] # This is a more straightforward way of deleting the undesirable variable and avoid mistakes due to typos when writing down the name of the column.

# # # #
table(malawi$hh_t08)</code></pre>
<pre><code>## 
##                                                                     
##                                                                   3 
##                                    ALLOWS YOU TO BUILD YOUR SAVINGS 
##                                                                2443 
##                                    ALLOWS YOU TO SAVE JUST A LITTLE 
##                                                                5762 
## IS NOT SUFFICIENT, SO YOU NEED TO USE YOUR SAVINGS TO MEET EXPENSES 
##                                                                8147 
##    IS REALLY NOT SUFFICIENT, SO YOU NEED TO BORROW TO MEET EXPENSES 
##                                                                7760 
##                                       ONLY JUST MEETS YOUR EXPENSES 
##                                                               17315</code></pre>
<pre class="r"><code>malawi$hh_t08 &lt;- droplevels(malawi$hh_t08, &quot;&quot;)

# # # #
table(malawi$hh_t01)</code></pre>
<pre><code>## 
##                                               
##                                             3 
##      It was just adequate for household needs 
##                                         13520 
## It was less than adequate for household needs 
##                                         25794 
## It was more than adequate for household needs 
##                                          2113</code></pre>
<pre class="r"><code>malawi$hh_t01 &lt;- droplevels(malawi$hh_t01, &quot;&quot;)

# # # #
table(malawi$hh_t14)</code></pre>
<pre><code>## 
##            DON&#39;T KNOW         NO        YES 
##          3         25       7949      33453</code></pre>
<pre class="r"><code>malawi$hh_t14 &lt;- droplevels(malawi$hh_t14, &quot;&quot;)

# Let&#39;s quickly check if we have any other factor variables with missing data:

malawi_factor2 &lt;- malawi %&gt;% select_if(~is.factor(.)) 
llply(.data=malawi_factor2, .fun=table)</code></pre>
<pre><code>## $reside
## 
## RURAL URBAN 
## 35814  5616 
## 
## $hh_s01
## 
##    NO   YES 
## 28946 12484 
## 
## $hh_b03
## 
## FEMALE   MALE 
##  21501  19929 
## 
## $hh_c09
## 
##   A-LEVEL    DEGREE   DIPLOMA       JCE   MASTERS MSCE/GCSE      NONE       PhD 
##       115       104       263      1794        23      1677     24110         4 
##      PSLC 
##      3328 
## 
## $hh_c24
## 
##    No   Yes 
## 34279  1496 
## 
## $hh_f19
## 
##    NO   YES 
## 36824  4606 
## 
## $hh_t08
## 
##                                    ALLOWS YOU TO BUILD YOUR SAVINGS 
##                                                                2443 
##                                    ALLOWS YOU TO SAVE JUST A LITTLE 
##                                                                5762 
## IS NOT SUFFICIENT, SO YOU NEED TO USE YOUR SAVINGS TO MEET EXPENSES 
##                                                                8147 
##    IS REALLY NOT SUFFICIENT, SO YOU NEED TO BORROW TO MEET EXPENSES 
##                                                                7760 
##                                       ONLY JUST MEETS YOUR EXPENSES 
##                                                               17315 
## 
## $hh_t01
## 
##      It was just adequate for household needs 
##                                         13520 
## It was less than adequate for household needs 
##                                         25794 
## It was more than adequate for household needs 
##                                          2113 
## 
## $hh_t14
## 
## DON&#39;T KNOW         NO        YES 
##         25       7949      33453</code></pre>
<p>Now that we have finally rid the entire data set from all missing
values, we can proceed to improve our model with</p>
<ol style="list-style-type: lower-alpha">
<li><p>Data-specific knowledge: Currently, we have a Food Consumption
variable per household. However, our dataset is not at the household
level, but at the user level. This means that, for each user, we have
the household spending, but not the share that would go to them.
Perhaps, a better alternative would be to create a new target variable
which corresponds to the users’ Food Consumption, and not their
household consumption.</p></li>
<li><p>Subject-specific knowledge: Usually, poverty measures look at
daily spending on food, or income per day to assess whether a person has
enough to cover their necessities. This <a
href="https://www.bbc.com/news/magazine-17312819">BBC article</a> tells
us the story of how World Bank Economist Martin Ravallion came up with
this measure and why it matters.</p></li>
</ol>
<pre class="r"><code># What is the daily, food Consumption per capita?
malawi$Cons_pcpd &lt;-(malawi$sumConsumption / malawi$hhsize)/7

summary(malawi$Cons_pcpd)</code></pre>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
##   0.0000   0.9881   1.6518   4.0861   2.9643 314.1429</code></pre>
<p>Note that the median person in Malawi lives with a little over 1 USD
per day. This seems consistent with World Bank <a
href="https://opportunity.org/our-impact/where-we-work/malawi-facts-about-poverty">records</a>.
In 2017, ca. 70 per cent of Malawians were living under the
international poverty line of <span
class="math inline">\(1.90/day\)</span>.</p>
<p>Now, let’s proceed to run our model and assess it:</p>
<pre class="r"><code>#Set a seed
set.seed(1234777)

# Split the data 

train_idx &lt;- createDataPartition(malawi$Cons_pcpd, p = .8, list = FALSE, times = 1)

Train_df &lt;- malawi[ train_idx,]
Test_df  &lt;- malawi[-train_idx,]

# Run our model

model_corrected &lt;- lm(Cons_pcpd ~ .,Train_df)</code></pre>
<pre class="r"><code>stargazer::stargazer(model_corrected, type = &quot;text&quot;)</code></pre>
<pre><code>## 
## =======================================================================================================
##                                                                                Dependent variable:     
##                                                                           -----------------------------
##                                                                                     Cons_pcpd          
## -------------------------------------------------------------------------------------------------------
## resideURBAN                                                                         0.324***           
##                                                                                      (0.125)           
##                                                                                                        
## hhsize                                                                              -0.919***          
##                                                                                      (0.018)           
##                                                                                                        
## hh_b05a                                                                             0.008***           
##                                                                                      (0.002)           
##                                                                                                        
## sumConsumption                                                                      0.034***           
##                                                                                     (0.0002)           
##                                                                                                        
## hh_s01YES                                                                            -0.137            
##                                                                                      (0.085)           
##                                                                                                        
## hh_b03MALE                                                                          0.277***           
##                                                                                      (0.078)           
##                                                                                                        
## hh_c09DEGREE                                                                          1.372            
##                                                                                      (0.933)           
##                                                                                                        
## hh_c09DIPLOMA                                                                       3.088***           
##                                                                                      (0.769)           
##                                                                                                        
## hh_c09JCE                                                                             0.299            
##                                                                                      (0.661)           
##                                                                                                        
## hh_c09MASTERS                                                                       7.614***           
##                                                                                      (1.880)           
##                                                                                                        
## hh_c09MSCE/GCSE                                                                      1.324**           
##                                                                                      (0.657)           
##                                                                                                        
## hh_c09NONE                                                                           -0.245            
##                                                                                      (0.650)           
##                                                                                                        
## hh_c09PhD                                                                            -2.040            
##                                                                                      (6.149)           
##                                                                                                        
## hh_c09PSLC                                                                            0.081            
##                                                                                      (0.655)           
##                                                                                                        
## hh_c24Yes                                                                             0.273            
##                                                                                      (0.221)           
##                                                                                                        
## hh_d10                                                                              -0.00003           
##                                                                                     (0.00003)          
##                                                                                                        
## hh_d11                                                                               0.0001*           
##                                                                                     (0.00003)          
##                                                                                                        
## hh_d12_1                                                                             0.00000           
##                                                                                     (0.00000)          
##                                                                                                        
## hh_f19YES                                                                            -0.121            
##                                                                                      (0.146)           
##                                                                                                        
## hh_f34                                                                              -0.456***          
##                                                                                      (0.043)           
##                                                                                                        
## hh_t08ALLOWS YOU TO SAVE JUST A LITTLE                                               -0.101            
##                                                                                      (0.188)           
##                                                                                                        
## hh_t08IS NOT SUFFICIENT, SO YOU NEED TO USE YOUR SAVINGS TO MEET EXPENSES             0.200            
##                                                                                      (0.189)           
##                                                                                                        
## hh_t08IS REALLY NOT SUFFICIENT, SO YOU NEED TO BORROW TO MEET EXPENSES               0.344*            
##                                                                                      (0.194)           
##                                                                                                        
## hh_t08ONLY JUST MEETS YOUR EXPENSES                                                   0.187            
##                                                                                      (0.175)           
##                                                                                                        
## hh_t01It was less than adequate for household needs                                  -0.180*           
##                                                                                      (0.093)           
##                                                                                                        
## hh_t01It was more than adequate for household needs                                  -0.268            
##                                                                                      (0.181)           
##                                                                                                        
## hh_t14NO                                                                             2.479*            
##                                                                                      (1.446)           
##                                                                                                        
## hh_t14YES                                                                            2.401*            
##                                                                                      (1.442)           
##                                                                                                        
## Constant                                                                              2.430            
##                                                                                      (1.598)           
##                                                                                                        
## -------------------------------------------------------------------------------------------------------
## Observations                                                                         25,103            
## R2                                                                                    0.676            
## Adjusted R2                                                                           0.676            
## Residual Std. Error                                                            6.112 (df = 25074)      
## F Statistic                                                               1,867.736*** (df = 28; 25074)
## =======================================================================================================
## Note:                                                                       *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<p>Note how the <em>R^2</em> has now significantly increased to <span
class="math inline">\(0.67\)</span>! Let’s not get our hopes up. A big
driver of this increase is the fact that the variable, weekly household
spending on food (our initial target variable), is used as a predictor.
Notice also that there is a zero variance predictor in the model. We
won’t dwell on the latter and focus on the former.</p>
<pre class="r"><code>Train_df &lt;- Train_df[,-which(colnames(Train_df)==&quot;sumConsumption&quot;)]
Test_df  &lt;- Test_df[,-which(colnames(Test_df)==&quot;sumConsumption&quot;)]

# This is a more direct way of deleting columns than what we had previously done. Just for your information, sumConsumption was column/vector number four in both dataframes, so we could have just removed column for instead of using the which() statement. 

model_corrected &lt;- lm(Cons_pcpd ~ .,Train_df)
stargazer(model_corrected, type = &quot;text&quot;)</code></pre>
<pre><code>## 
## =====================================================================================================
##                                                                               Dependent variable:    
##                                                                           ---------------------------
##                                                                                    Cons_pcpd         
## -----------------------------------------------------------------------------------------------------
## resideURBAN                                                                        0.767***          
##                                                                                     (0.209)          
##                                                                                                      
## hhsize                                                                             -0.863***         
##                                                                                     (0.030)          
##                                                                                                      
## hh_b05a                                                                              0.006           
##                                                                                     (0.004)          
##                                                                                                      
## hh_s01YES                                                                            0.217           
##                                                                                     (0.142)          
##                                                                                                      
## hh_b03MALE                                                                         0.406***          
##                                                                                     (0.129)          
##                                                                                                      
## hh_c09DEGREE                                                                         0.749           
##                                                                                     (1.555)          
##                                                                                                      
## hh_c09DIPLOMA                                                                      6.510***          
##                                                                                     (1.281)          
##                                                                                                      
## hh_c09JCE                                                                            1.795           
##                                                                                     (1.102)          
##                                                                                                      
## hh_c09MASTERS                                                                      8.829***          
##                                                                                     (3.133)          
##                                                                                                      
## hh_c09MSCE/GCSE                                                                     2.723**          
##                                                                                     (1.096)          
##                                                                                                      
## hh_c09NONE                                                                           0.814           
##                                                                                     (1.083)          
##                                                                                                      
## hh_c09PhD                                                                            4.474           
##                                                                                    (10.247)          
##                                                                                                      
## hh_c09PSLC                                                                           1.259           
##                                                                                     (1.091)          
##                                                                                                      
## hh_c24Yes                                                                          3.203***          
##                                                                                     (0.368)          
##                                                                                                      
## hh_d10                                                                              0.0001           
##                                                                                    (0.00004)         
##                                                                                                      
## hh_d11                                                                              0.00001          
##                                                                                    (0.00005)         
##                                                                                                      
## hh_d12_1                                                                            0.00000          
##                                                                                    (0.00000)         
##                                                                                                      
## hh_f19YES                                                                          2.350***          
##                                                                                     (0.243)          
##                                                                                                      
## hh_f34                                                                             0.370***          
##                                                                                     (0.072)          
##                                                                                                      
## hh_t08ALLOWS YOU TO SAVE JUST A LITTLE                                             -1.009***         
##                                                                                     (0.313)          
##                                                                                                      
## hh_t08IS NOT SUFFICIENT, SO YOU NEED TO USE YOUR SAVINGS TO MEET EXPENSES          -1.912***         
##                                                                                     (0.315)          
##                                                                                                      
## hh_t08IS REALLY NOT SUFFICIENT, SO YOU NEED TO BORROW TO MEET EXPENSES             -1.427***         
##                                                                                     (0.322)          
##                                                                                                      
## hh_t08ONLY JUST MEETS YOUR EXPENSES                                                -1.522***         
##                                                                                     (0.292)          
##                                                                                                      
## hh_t01It was less than adequate for household needs                                -0.586***         
##                                                                                     (0.155)          
##                                                                                                      
## hh_t01It was more than adequate for household needs                                1.782***          
##                                                                                     (0.302)          
##                                                                                                      
## hh_t14NO                                                                            -3.098           
##                                                                                     (2.409)          
##                                                                                                      
## hh_t14YES                                                                           -4.538*          
##                                                                                     (2.403)          
##                                                                                                      
## Constant                                                                           12.561***         
##                                                                                     (2.662)          
##                                                                                                      
## -----------------------------------------------------------------------------------------------------
## Observations                                                                        25,103           
## R2                                                                                   0.100           
## Adjusted R2                                                                          0.099           
## Residual Std. Error                                                           10.186 (df = 25075)    
## F Statistic                                                               102.942*** (df = 27; 25075)
## =====================================================================================================
## Note:                                                                     *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<p>Unfortunately, once we remove the weekly food consumption variable,
from which the daily, per capita food consumption was derived, our R^2
drops down to similar levels as before: <span
class="math inline">\(0.10\)</span>, ca. ten per cent of the variance of
our model explained.</p>
<p>What woule happen if we also got rid of the zero-variance predictor?
As a reminder, the variable “hh_d12_1” measures: How much in total did
[NAME] spend..for medical insurance?</p>
<pre class="r"><code>Train_df &lt;- Train_df[,-which(colnames(Train_df)==&quot;hh_d12_1&quot;)]
Test_df  &lt;- Test_df[,-which(colnames(Test_df)==&quot;hh_d12_1&quot;)]


model_corrected &lt;- lm(Cons_pcpd ~ .,Train_df)
stargazer(model_corrected, type = &quot;text&quot;)</code></pre>
<pre><code>## 
## =====================================================================================================
##                                                                               Dependent variable:    
##                                                                           ---------------------------
##                                                                                    Cons_pcpd         
## -----------------------------------------------------------------------------------------------------
## resideURBAN                                                                        0.767***          
##                                                                                     (0.209)          
##                                                                                                      
## hhsize                                                                             -0.863***         
##                                                                                     (0.030)          
##                                                                                                      
## hh_b05a                                                                              0.006           
##                                                                                     (0.004)          
##                                                                                                      
## hh_s01YES                                                                            0.217           
##                                                                                     (0.142)          
##                                                                                                      
## hh_b03MALE                                                                         0.406***          
##                                                                                     (0.129)          
##                                                                                                      
## hh_c09DEGREE                                                                         0.841           
##                                                                                     (1.552)          
##                                                                                                      
## hh_c09DIPLOMA                                                                      6.513***          
##                                                                                     (1.281)          
##                                                                                                      
## hh_c09JCE                                                                            1.791           
##                                                                                     (1.102)          
##                                                                                                      
## hh_c09MASTERS                                                                      9.959***          
##                                                                                     (2.893)          
##                                                                                                      
## hh_c09MSCE/GCSE                                                                     2.720**          
##                                                                                     (1.096)          
##                                                                                                      
## hh_c09NONE                                                                           0.813           
##                                                                                     (1.083)          
##                                                                                                      
## hh_c09PhD                                                                            4.477           
##                                                                                    (10.247)          
##                                                                                                      
## hh_c09PSLC                                                                           1.257           
##                                                                                     (1.091)          
##                                                                                                      
## hh_c24Yes                                                                          3.208***          
##                                                                                     (0.368)          
##                                                                                                      
## hh_d10                                                                             0.0001**          
##                                                                                    (0.00004)         
##                                                                                                      
## hh_d11                                                                              0.00001          
##                                                                                    (0.00005)         
##                                                                                                      
## hh_f19YES                                                                          2.348***          
##                                                                                     (0.243)          
##                                                                                                      
## hh_f34                                                                             0.370***          
##                                                                                     (0.072)          
##                                                                                                      
## hh_t08ALLOWS YOU TO SAVE JUST A LITTLE                                             -1.011***         
##                                                                                     (0.313)          
##                                                                                                      
## hh_t08IS NOT SUFFICIENT, SO YOU NEED TO USE YOUR SAVINGS TO MEET EXPENSES          -1.914***         
##                                                                                     (0.315)          
##                                                                                                      
## hh_t08IS REALLY NOT SUFFICIENT, SO YOU NEED TO BORROW TO MEET EXPENSES             -1.429***         
##                                                                                     (0.322)          
##                                                                                                      
## hh_t08ONLY JUST MEETS YOUR EXPENSES                                                -1.525***         
##                                                                                     (0.292)          
##                                                                                                      
## hh_t01It was less than adequate for household needs                                -0.586***         
##                                                                                     (0.155)          
##                                                                                                      
## hh_t01It was more than adequate for household needs                                1.790***          
##                                                                                     (0.302)          
##                                                                                                      
## hh_t14NO                                                                            -3.094           
##                                                                                     (2.409)          
##                                                                                                      
## hh_t14YES                                                                           -4.535*          
##                                                                                     (2.403)          
##                                                                                                      
## Constant                                                                           12.561***         
##                                                                                     (2.662)          
##                                                                                                      
## -----------------------------------------------------------------------------------------------------
## Observations                                                                        25,103           
## R2                                                                                   0.100           
## Adjusted R2                                                                          0.099           
## Residual Std. Error                                                           10.186 (df = 25076)    
## F Statistic                                                               106.867*** (df = 26; 25076)
## =====================================================================================================
## Note:                                                                     *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<p>Not much. The R^2 remains the same.</p>
<p>Let’s take a look at the in-sample RMSE now:</p>
<pre class="r"><code># 1. Predict values on the training data set
p00 &lt;- predict(model_corrected, Train_df)

# 2. Obtain the errors (predicted values minus observed values of target variable)
error00 &lt;- p00 - Train_df[[&quot;Cons_pcpd&quot;]]

# 3. In-sample RMSE
print(RMSE_insample_correctedmodel &lt;- sqrt(mean(error00 ^ 2, na.rm = T)))</code></pre>
<pre><code>## [1] 10.18081</code></pre>
<p>Despite our R^2 value being low, which, again, is not uncommon in the
social sciences, our in-sample RMSE is also low! We have managed to
decrease the bias in the model significantly from <span
class="math inline">\(228.4\)</span>, all the way down to <span
class="math inline">\(10.18\)</span>. Let’s see if we observe a similar
trend in the test data set with the out of sample RMSE.</p>
<pre class="r"><code># 1. Predict values on the training data set
p00 &lt;- predict(model_corrected, Test_df)

# 2. Obtain the errors (predicted values minus observed values of target variable)
error00 &lt;- p00 - Test_df[[&quot;Cons_pcpd&quot;]]

# 3. In-sample RMSE
print(RMSE_outofsample_correctedmodel &lt;- sqrt(mean(error00 ^ 2, na.rm = T)))</code></pre>
<pre><code>## [1] 9.427004</code></pre>
<p>With an out of sample RMSE of <span
class="math inline">\(10.18\)</span>, and an in-sample RMSE of <span
class="math inline">\(9.42\)</span>, it seems like our model is
consistent across data sets. So, whilst we were not able to increase the
R^2, we did significantly decrease the bias in our model. This, to me,
is a success! As we learn more techniques, we will be able to build from
this - already highly improved model - to try to find the most accurate
prediction.</p>
</div>
</div>

<!DOCTYPE html>
<hr>
<p style="text-align: center;">Copyright &copy; 2022 <i class="fa-light fa-person-to-portal"></i> Michelle González Amador & Stephan Dietrich <i class="fa-light fa-person-from-portal"></i>. All rights reserved.</p>
<p style="text-align: center;"><a href="https://github.com/michelleg06/Machine-Learning-for-Public-Policy" class="fa fa-github"></a></p>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
