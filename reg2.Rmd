---
<<<<<<< HEAD
title: "Supervised Machine Learning: Lasso and Ridge shrinkage methods (Regression II)"
=======
title: "Supervised Machine Learning: Linear Models (Regression II)"
>>>>>>> 8a3ac542642ede74c728d8ec3b855785f2a65632
#author: "Dr. Stephan Dietrich & Michelle Gonz√°lez Amador"
#date: '2022-09-21'
output: html_document
---

<<<<<<< HEAD
An OLS regression is not the only model that can be written in the form of $Y_i = \alpha + \beta_1X_{1i}, \beta_2X_{2i},..., \beta_pX_{pi}+ u_i$. In this section we will discuss "penalised" models, which can also be expressed as a linear relationship between parameters. Penalised regression models are also known as regression shrinkage methods, and they take their name after the colloquial term for coefficient regularisation, "shrinkage" of estimated coefficients. The goal of penalised models, as opposed to a traditional linear model, is not to minimise bias, but to reduce variance by adding a constraint to the equation and effectively pushing coefficient parameters towards $0$. This results in the the worse model predictors having a coefficient of zero or close to zero. 


**Lasso**

Consider a scenario where you have dozens (maybe thousands?) of predictors. Which covariates are truly important for our known outcome? Including all of the predictors leads to *over-fitting*. We'll find that the R^2 value is high, and conclude that our in-sample fit is good. However, this may lead to bad out-of-sample predictions. *Model selection* is a particularly challenging endeavour when we encounter high-dimensional data: when the number of variables is close to or larger than the number of observations. Some examples where you may encounter high-dimensional data include:

1. Cross-country analyses: we have a small and finite number of countries, but we may collect/observe as many variables as we want. 

2. Cluster-population analyses: we wish to understand the outcome of some unique population $n$, e.g. all students from classroom A. We collect plenty of information on these students, but the sample and the population are analogous $n = N$, and thus the sample number of observations is small and finite. 

The LASSO - Least Absolute Shrinkage and Selection Operator imposes a shrinking penalty to those predictors that do not actually belong in the model, and reduces the size of the estimated $\beta$ coefficients towards and including zero (when the tuning parameter / shrinkage penalty $\lambda$ is sufficiently large). Note that $lambda$ is the penalty term called *L1-norm*, and corresponds to the sum of the absolute coefficients. 

**Ridge**

A ridge regression includes ALL predictors in a model, but penalises predictors that contribute less to the model by shrinking them close to (but never) zero. The penalty term $\lambda$ in a ridge regression is called the *L2-norm* and is the sum of the squared coefficients. The ridge regression is the predecessor to the lasso.

Selecting the value of $\lambda$ is critical for ridge regressions; when $\lambda = 0$, a ridge regression is essentially an OLS regression. As $\lambda \rightarrow \infty$, the penalty increases and the regression coefficients approximate zero.

Our practical excercise in R will consist of running a Lasso model using the caret package.



=======
Section under construction.
>>>>>>> 8a3ac542642ede74c728d8ec3b855785f2a65632
